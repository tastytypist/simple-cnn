{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional Neural Network\n",
    "\n",
    "# Simple CNN\n",
    "**Simple CNN** is a convolutional neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T10:10:27.852581300Z",
     "start_time": "2023-10-05T10:10:27.595520100Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T10:10:27.867888800Z",
     "start_time": "2023-10-05T10:10:27.858583300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9981df315bc56c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T10:10:27.925470Z",
     "start_time": "2023-10-05T10:10:27.863894500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._result = []\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def feed_forward(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing feed forward on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._biases = None\n",
    "\n",
    "        def _pad_weights(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "            padding_size: int,\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            self._weight_dimension = len(weights)\n",
    "\n",
    "            self._weight_height = (weight_height := len(weights[0])) + 2 * padding_size\n",
    "            self._weight_width = (weight_width := len(weights[0][0])) + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        weights[i][j - padding_size][k - padding_size]\n",
    "                        if padding_size <= j < weight_height + padding_size\n",
    "                        or padding_size <= k < weight_width + padding_size\n",
    "                        else 0.0\n",
    "                        for k in range(self._weight_width)\n",
    "                    ]\n",
    "                    for j in range(self._weight_height)\n",
    "                ]\n",
    "                for i in range(self._weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += feature\n",
    "            return feature_maps\n",
    "\n",
    "        def feed_forward(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().feed_forward()\n",
    "            result = self.convolute(weights)\n",
    "            print(\"Convolution result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "\n",
    "        @staticmethod\n",
    "        def detect(feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def feed_forward(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().feed_forward()\n",
    "            result = self.detect(feature)\n",
    "            print(\"Detector result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "\n",
    "        def average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self.average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self.max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def feed_forward(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().feed_forward()\n",
    "            result = self.pool(input_matrix)\n",
    "            print(\"Pooling result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._weight = np.random.randn(unit_count)\n",
    "\n",
    "        def dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                return expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                return np.maximum(result, 0)\n",
    "\n",
    "        def feed_forward(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().feed_forward()\n",
    "            result = self.dense(input_matrix)\n",
    "            print(\"Dense result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "\n",
    "        @staticmethod\n",
    "        def flatten(input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        def feed_forward(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().feed_forward()\n",
    "            result = self.flatten(input_matrix)\n",
    "            print(\"Flatten result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.feed_forward(tensor)\n",
    "        print(\"Forward propagation result: \")\n",
    "        print(tensor)\n",
    "        self._result = tensor\n",
    "\n",
    "    def backward_propagate(self) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, tensor: npt.NDArray[npt.NDArray]) -> None:\n",
    "        \"\"\"\n",
    "        This is a docstring placeholder.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fb427e45bb865f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T10:10:45.078593900Z",
     "start_time": "2023-10-05T10:10:27.925470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feed forward on convolution layer...\n",
      "Convolution result: \n",
      "[[[868.67895311 857.65894293 850.98398622 ... 817.08664045 814.9669372\n",
      "   818.90985305]\n",
      "  [856.72159158 849.28625018 847.59372375 ... 820.5190599  817.91896063\n",
      "   821.41735933]\n",
      "  [850.67003161 846.66462998 848.68294175 ... 822.27917848 818.82892326\n",
      "   821.92506377]\n",
      "  ...\n",
      "  [392.5603581  305.00897691 194.6305803  ...  42.0719063  140.1561758\n",
      "   202.73327111]\n",
      "  [299.34878771 193.45848893 123.57773941 ...  82.12676841 114.10526721\n",
      "   260.23328176]\n",
      "  [203.62017112  96.78928485 123.6802216  ... 128.09404718 121.13054147\n",
      "   304.48294263]]\n",
      "\n",
      " [[802.64427283 791.02732147 784.00713875 ... 755.50597162 751.92548236\n",
      "   753.02000558]\n",
      "  [791.1412021  781.55475272 779.79793888 ... 758.0910971  754.50072245\n",
      "   755.44261018]\n",
      "  [786.79750155 780.4942912  781.26060489 ... 759.03238033 754.8628902\n",
      "   755.01322804]\n",
      "  ...\n",
      "  [355.54730014 286.21557192 191.39981607 ...  35.41568151  85.95131728\n",
      "   181.50223021]\n",
      "  [304.24218612 182.63016375 122.12285428 ...  79.2862307   80.06993951\n",
      "   227.55058868]\n",
      "  [205.35607739  93.28888889 103.06017828 ... 117.10713176 103.20014499\n",
      "   259.4064786 ]]\n",
      "\n",
      " [[788.25440499 777.55394661 769.37765698 ... 737.04123473 736.04760766\n",
      "   738.41567698]\n",
      "  [777.76601735 770.28878403 765.90004465 ... 741.58205505 739.67425611\n",
      "   741.00999083]\n",
      "  [771.69111046 765.35497049 765.05195872 ... 742.70564865 741.2542219\n",
      "   742.08515461]\n",
      "  ...\n",
      "  [373.69943717 289.13875344 214.74412971 ...  40.70465097 128.02925339\n",
      "   153.59709049]\n",
      "  [285.15819228 201.5608314  132.00559629 ...  83.99147789  95.50654259\n",
      "   227.00765166]\n",
      "  [223.85933546 114.50160397 105.78912143 ... 105.38422646  82.74265755\n",
      "   269.60965873]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[758.21620468 748.28917051 742.65950855 ... 709.88972131 708.62194145\n",
      "   712.81366072]\n",
      "  [746.41216479 739.86652933 739.58719685 ... 714.12291645 712.28680076\n",
      "   716.01380743]\n",
      "  [740.06453627 736.65818285 738.81624615 ... 715.08000169 713.49557681\n",
      "   716.73366384]\n",
      "  ...\n",
      "  [332.4809447  268.48028244 173.06030454 ...  38.62256433 126.12120199\n",
      "   208.15772165]\n",
      "  [256.43655641 182.22563677 134.08505326 ...  69.06023864  94.96449681\n",
      "   244.39977318]\n",
      "  [192.36251751  84.95192661 114.13475647 ... 115.84376837  99.8262949\n",
      "   282.55773923]]\n",
      "\n",
      " [[579.38649026 571.48837416 566.48978622 ... 541.52090702 538.37426808\n",
      "   540.92625013]\n",
      "  [571.44151629 564.64283359 563.83682024 ... 544.36934648 541.88245934\n",
      "   543.5352123 ]\n",
      "  [565.42402438 560.92747557 562.04356336 ... 546.18953277 543.44786394\n",
      "   544.59473345]\n",
      "  ...\n",
      "  [253.00097097 201.36409901 146.80759725 ...  27.56941505  64.33797677\n",
      "   169.50225015]\n",
      "  [200.48167023 158.60656924 144.90280182 ...  49.67427266  61.88680065\n",
      "   184.13889057]\n",
      "  [186.15007034  83.40041171  63.31382514 ...  76.05139362  64.00615849\n",
      "   171.11041524]]\n",
      "\n",
      " [[838.28412412 828.44812335 822.47135253 ... 790.38050017 788.03377646\n",
      "   790.72654351]\n",
      "  [826.51876462 819.92761088 819.68075791 ... 792.76874981 790.65849111\n",
      "   793.49535717]\n",
      "  [822.50579652 818.66986041 819.95470515 ... 794.12283681 790.72523215\n",
      "   793.05685701]\n",
      "  ...\n",
      "  [349.61872011 271.95181528 184.66820041 ...  44.47126331 112.08793846\n",
      "   190.1022132 ]\n",
      "  [267.99937623 164.17962854 138.93903591 ...  79.70048343 110.60146383\n",
      "   254.08240514]\n",
      "  [200.46480688 106.62677461 126.97143558 ... 131.54038351 117.0961733\n",
      "   285.63728675]]]\n",
      "\n",
      "Performing feed forward on detector layer...\n",
      "\n",
      "Detector result: \n",
      "[[[868.67895311 857.65894293 850.98398622 ... 817.08664045 814.9669372\n",
      "   818.90985305]\n",
      "  [856.72159158 849.28625018 847.59372375 ... 820.5190599  817.91896063\n",
      "   821.41735933]\n",
      "  [850.67003161 846.66462998 848.68294175 ... 822.27917848 818.82892326\n",
      "   821.92506377]\n",
      "  ...\n",
      "  [392.5603581  305.00897691 194.6305803  ...  42.0719063  140.1561758\n",
      "   202.73327111]\n",
      "  [299.34878771 193.45848893 123.57773941 ...  82.12676841 114.10526721\n",
      "   260.23328176]\n",
      "  [203.62017112  96.78928485 123.6802216  ... 128.09404718 121.13054147\n",
      "   304.48294263]]\n",
      "\n",
      " [[802.64427283 791.02732147 784.00713875 ... 755.50597162 751.92548236\n",
      "   753.02000558]\n",
      "  [791.1412021  781.55475272 779.79793888 ... 758.0910971  754.50072245\n",
      "   755.44261018]\n",
      "  [786.79750155 780.4942912  781.26060489 ... 759.03238033 754.8628902\n",
      "   755.01322804]\n",
      "  ...\n",
      "  [355.54730014 286.21557192 191.39981607 ...  35.41568151  85.95131728\n",
      "   181.50223021]\n",
      "  [304.24218612 182.63016375 122.12285428 ...  79.2862307   80.06993951\n",
      "   227.55058868]\n",
      "  [205.35607739  93.28888889 103.06017828 ... 117.10713176 103.20014499\n",
      "   259.4064786 ]]\n",
      "\n",
      " [[788.25440499 777.55394661 769.37765698 ... 737.04123473 736.04760766\n",
      "   738.41567698]\n",
      "  [777.76601735 770.28878403 765.90004465 ... 741.58205505 739.67425611\n",
      "   741.00999083]\n",
      "  [771.69111046 765.35497049 765.05195872 ... 742.70564865 741.2542219\n",
      "   742.08515461]\n",
      "  ...\n",
      "  [373.69943717 289.13875344 214.74412971 ...  40.70465097 128.02925339\n",
      "   153.59709049]\n",
      "  [285.15819228 201.5608314  132.00559629 ...  83.99147789  95.50654259\n",
      "   227.00765166]\n",
      "  [223.85933546 114.50160397 105.78912143 ... 105.38422646  82.74265755\n",
      "   269.60965873]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[758.21620468 748.28917051 742.65950855 ... 709.88972131 708.62194145\n",
      "   712.81366072]\n",
      "  [746.41216479 739.86652933 739.58719685 ... 714.12291645 712.28680076\n",
      "   716.01380743]\n",
      "  [740.06453627 736.65818285 738.81624615 ... 715.08000169 713.49557681\n",
      "   716.73366384]\n",
      "  ...\n",
      "  [332.4809447  268.48028244 173.06030454 ...  38.62256433 126.12120199\n",
      "   208.15772165]\n",
      "  [256.43655641 182.22563677 134.08505326 ...  69.06023864  94.96449681\n",
      "   244.39977318]\n",
      "  [192.36251751  84.95192661 114.13475647 ... 115.84376837  99.8262949\n",
      "   282.55773923]]\n",
      "\n",
      " [[579.38649026 571.48837416 566.48978622 ... 541.52090702 538.37426808\n",
      "   540.92625013]\n",
      "  [571.44151629 564.64283359 563.83682024 ... 544.36934648 541.88245934\n",
      "   543.5352123 ]\n",
      "  [565.42402438 560.92747557 562.04356336 ... 546.18953277 543.44786394\n",
      "   544.59473345]\n",
      "  ...\n",
      "  [253.00097097 201.36409901 146.80759725 ...  27.56941505  64.33797677\n",
      "   169.50225015]\n",
      "  [200.48167023 158.60656924 144.90280182 ...  49.67427266  61.88680065\n",
      "   184.13889057]\n",
      "  [186.15007034  83.40041171  63.31382514 ...  76.05139362  64.00615849\n",
      "   171.11041524]]\n",
      "\n",
      " [[838.28412412 828.44812335 822.47135253 ... 790.38050017 788.03377646\n",
      "   790.72654351]\n",
      "  [826.51876462 819.92761088 819.68075791 ... 792.76874981 790.65849111\n",
      "   793.49535717]\n",
      "  [822.50579652 818.66986041 819.95470515 ... 794.12283681 790.72523215\n",
      "   793.05685701]\n",
      "  ...\n",
      "  [349.61872011 271.95181528 184.66820041 ...  44.47126331 112.08793846\n",
      "   190.1022132 ]\n",
      "  [267.99937623 164.17962854 138.93903591 ...  79.70048343 110.60146383\n",
      "   254.08240514]\n",
      "  [200.46480688 106.62677461 126.97143558 ... 131.54038351 117.0961733\n",
      "   285.63728675]]]\n",
      "\n",
      "Performing feed forward on pooling layer...\n",
      "Pooling result: \n",
      "[[[852.99345012 849.81817471 852.10369674 ... 823.88264841 822.07816044\n",
      "   819.97705506]\n",
      "  [851.060709   853.68437874 855.53445984 ... 831.54444756 828.82453709\n",
      "   824.65448909]\n",
      "  [849.61557027 852.07422756 850.26725747 ... 835.83030592 831.87209809\n",
      "   825.87047995]\n",
      "  ...\n",
      "  [273.46910407 327.4377365  373.08486713 ... 246.31267262 176.93169936\n",
      "   135.28884058]\n",
      "  [399.4865071  312.41205616 280.06671992 ... 299.96468462 179.97425766\n",
      "   101.50612131]\n",
      "  [295.94162133 218.50345688 227.39323409 ... 245.97912589 165.7838258\n",
      "    91.18803829]]\n",
      "\n",
      " [[786.52500271 783.41467644 785.31706905 ... 760.36502686 757.71114546\n",
      "   756.6073434 ]\n",
      "  [784.68379461 786.66416788 788.21073218 ... 767.21012627 764.12952546\n",
      "   760.86297105]\n",
      "  [782.81061752 785.11134082 783.55341814 ... 770.66415365 766.90542991\n",
      "   761.90671434]\n",
      "  ...\n",
      "  [255.58420651 299.5555014  335.10687714 ... 234.85553606 173.60366154\n",
      "   117.2925526 ]\n",
      "  [371.1863093  298.45017427 247.06343707 ... 279.45615988 188.45554027\n",
      "    86.41746926]\n",
      "  [277.23837253 195.67722078 210.32290575 ... 220.19656868 171.19515157\n",
      "    76.12198988]]\n",
      "\n",
      " [[772.35987714 767.37033399 768.73774682 ... 743.76315104 741.75060517\n",
      "   740.41441284]\n",
      "  [768.95071204 770.00682794 772.31515312 ... 750.71676795 747.96742508\n",
      "   744.86486735]\n",
      "  [768.08099753 769.40847765 768.6178937  ... 754.79161461 751.50226355\n",
      "   746.73738395]\n",
      "  ...\n",
      "  [239.11242983 285.14970072 333.71156589 ... 210.24400026 163.81256245\n",
      "   121.13348461]\n",
      "  [348.96160357 295.0330932  255.64059659 ... 263.12383226 172.08487922\n",
      "    91.32113912]\n",
      "  [284.92794596 208.73052225 203.46799007 ... 216.42397641 158.40273942\n",
      "    85.10911952]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[743.39663778 740.27681312 741.76414733 ... 716.73168797 715.56197046\n",
      "   713.60340283]\n",
      "  [740.77489625 743.50575498 745.39608668 ... 723.41022944 721.15666468\n",
      "   717.42163313]\n",
      "  [740.72952074 742.6055476  740.99179546 ... 727.87994698 724.78589112\n",
      "   719.0087053 ]\n",
      "  ...\n",
      "  [225.71722765 276.51328722 333.71838293 ... 201.87216887 145.6419552\n",
      "   124.03053536]\n",
      "  [346.55772346 273.62976645 249.90139867 ... 264.11404051 146.27249909\n",
      "    96.06970093]\n",
      "  [266.45911377 194.54826132 196.9486002  ... 224.69331914 136.08055814\n",
      "    78.58934397]]\n",
      "\n",
      " [[567.29787601 563.76416971 564.279543   ... 545.79881255 544.5996223\n",
      "   543.57408507]\n",
      "  [564.04121048 565.79074714 567.62391185 ... 551.3683727  549.24673714\n",
      "   546.85673188]\n",
      "  [564.17891052 565.73443965 564.74805275 ... 554.57753286 552.18186829\n",
      "   548.27285902]\n",
      "  ...\n",
      "  [168.06708424 194.34760871 251.16348602 ... 149.73920055 113.88164093\n",
      "    94.73741114]\n",
      "  [259.39065576 217.03324881 190.0376233  ... 206.35632016 123.00204856\n",
      "    70.56748581]\n",
      "  [216.89257743 153.35761669 151.23973287 ... 176.11510051 117.83175822\n",
      "    52.64471379]]\n",
      "\n",
      " [[824.05123283 821.64845713 823.83584583 ... 796.08244815 794.30254441\n",
      "   792.44786495]\n",
      "  [822.21029782 824.34090079 825.80452095 ... 803.66656667 800.7152659\n",
      "   796.71603559]\n",
      "  [820.6803618  822.55528678 820.82812534 ... 806.93707844 803.52505691\n",
      "   797.64553834]\n",
      "  ...\n",
      "  [281.38632269 306.76024998 354.83042389 ... 242.46719051 162.68939001\n",
      "   129.1516835 ]\n",
      "  [379.03900574 297.60186553 260.89558145 ... 299.07108306 175.1927877\n",
      "    95.15901967]\n",
      "  [272.03481341 220.44667317 211.39379685 ... 233.77587928 167.32623521\n",
      "    83.70155165]]]\n",
      "\n",
      "Performing feed forward on flatten layer...\n",
      "\n",
      "Flatten result: \n",
      "[852.99345012 849.81817471 852.10369674 ... 233.77587928 167.32623521\n",
      "  83.70155165]\n",
      "\n",
      "Performing feed forward on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[0.00000000e+00 0.00000000e+00 2.50665217e+08 2.95526533e+08\n",
      " 0.00000000e+00 4.25054458e+08 0.00000000e+00 3.71061852e+08]\n",
      "\n",
      "Performing feed forward on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[1.]\n",
      "\n",
      "Forward propagation result: \n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path)\n",
    "image_number = 0\n",
    "image_matrix = [image_matrix[image_number]]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.forward_propagate(image_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
