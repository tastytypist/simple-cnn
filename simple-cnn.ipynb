{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional Neural Network\n",
    "\n",
    "# Simple CNN\n",
    "**Simple CNN** is a convolutional neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.241130300Z",
     "start_time": "2023-10-05T11:04:52.944944900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.253382100Z",
     "start_time": "2023-10-05T11:04:53.246131700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model. In\n",
    "    addition, this class is also used to save and load the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model_object: \"Model\", file_name: str = \"model.json\") -> None:\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json = jsonpickle.encode(model_object, indent=4)\n",
    "            file.write(json)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str = \"model.json\") -> \"Model\":\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"r\") as file:\n",
    "            json = file.read()\n",
    "            return jsonpickle.decode(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9981df315bc56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.315203200Z",
     "start_time": "2023-10-05T11:04:53.302381400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._result = []\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def forward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing forward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the backward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing backward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._biases = None\n",
    "\n",
    "        def _pad_weights(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "            padding_size: int,\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            self._weight_dimension = len(weights)\n",
    "\n",
    "            self._weight_height = (weight_height := len(weights[0])) + 2 * padding_size\n",
    "            self._weight_width = (weight_width := len(weights[0][0])) + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        weights[i][j - padding_size][k - padding_size]\n",
    "                        if padding_size <= j < weight_height + padding_size\n",
    "                        or padding_size <= k < weight_width + padding_size\n",
    "                        else 0.0\n",
    "                        for k in range(self._weight_width)\n",
    "                    ]\n",
    "                    for j in range(self._weight_height)\n",
    "                ]\n",
    "                for i in range(self._weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += np.sum(feature)\n",
    "            return feature_maps\n",
    "\n",
    "        def forward_propagate(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.convolute(weights)\n",
    "            print(\"Convolution result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "\n",
    "        @staticmethod\n",
    "        def detect(feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def forward_propagate(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.detect(feature)\n",
    "            print(\"Detector result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "\n",
    "        def average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self.average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self.max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.pool(input_matrix)\n",
    "            print(\"Pooling result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def backward(self, error):\n",
    "            # F : filter, W : width, H : height\n",
    "            F, W, H = self.input.shape\n",
    "            dX = np.zeros(self.input.shape)\n",
    "            for i in range(0, F):\n",
    "                for j in range(0, W, self._filter_size):\n",
    "                    for k in range(0, H, self._filter_size):\n",
    "                        #slice input and find max from slice\n",
    "                        input_slice = self.input[i, j : j + self._filter_size, k : k + self._filter_size]\n",
    "                        max_input_slice = np.argmax(input_slice)\n",
    "\n",
    "                        #Find index for maximum value in slice\n",
    "                        max_idx = np.unravel_index(max_input_slice, (self._filter_size, self._filter_size))\n",
    "\n",
    "                        #update dX value\n",
    "                        if ((j + max_idx[0]) < W and (k + max_idx[1]) < H):\n",
    "                            dX[i, j + max_idx[0], k + max_idx[1]] = error[i, int(j // self._filter_size) , int(k // self._filter_size)]\n",
    "            \n",
    "            return dX\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._weight = []\n",
    "            self._deltaW = np.zeros((unit_count))\n",
    "\n",
    "        def dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            if(len(self._weight) == 0):\n",
    "                self._weight = np.random.randn(self._unit_count,len(self.input))\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                self.output =  expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                self.output = np.maximum(result, 0)\n",
    "            return self.output\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.dense(input_matrix)\n",
    "            print(\"Dense result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        def backward(self, error):\n",
    "            derivative_value = np.array([])\n",
    "            for i in self.output:\n",
    "                derivative_value = np.append(derivative_value, self.derivative_act_func(self._activation, i))\n",
    "            \n",
    "            self._deltaW += np.multiply(derivative_value, error)\n",
    "            dE = np.matmul(error, self._weight)\n",
    "            return dE\n",
    "        \n",
    "        def update_weight(self, learning_rate, momentum):\n",
    "            \"\"\" \n",
    "            Indicate and perform the update weight and bias on the model\n",
    "            \"\"\"\n",
    "            for i in range(self._unit_count):\n",
    "                self._weight[i] = self._weight[i] - ((momentum * self._weight[i]) + (learning_rate * self._deltaW[i] * self.input))\n",
    "\n",
    "            self._bias = self._bias - ((momentum * self._bias) + (learning_rate * self._deltaW))\n",
    "            self._deltaW = np.zeros((self._unit_count))\n",
    "\n",
    "        def derivative_act_func(self, activation, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from activation function and input\n",
    "            \"\"\"\n",
    "            if(activation == \"sigmoid\"):\n",
    "                return self.sigmoid_detivative(input)\n",
    "            else:\n",
    "                return self.relu_derivative(input)\n",
    "            \n",
    "        def sigmoid_detivative(self, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            sigmoid = 1/(1+np.exp(-input))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        \n",
    "        def relu_derivative(self, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            if(input >= 0):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "\n",
    "        @staticmethod\n",
    "        def flatten(input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.flatten(input_matrix)\n",
    "            print(\"Flatten result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.forward_propagate(tensor)\n",
    "        print(\"Forward propagation result: \")\n",
    "        print(tensor)\n",
    "        self._result = tensor\n",
    "\n",
    "    def backward_propagate(self) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, tensor: npt.NDArray[npt.NDArray], epochs: int = 3, learning_rate: float = 0.01) -> None:\n",
    "        \"\"\"\n",
    "        Fit and train the CNN model.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        :param epochs: An integer specifying the number of training epochs.\n",
    "        :param learning_rate: A float specifying the learning rate of the model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80fb427e45bb865f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:05:10.033906700Z",
     "start_time": "2023-10-05T11:04:53.312209300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on convolution layer...\n",
      "\n",
      "Convolution result: \n",
      "[[[ 792.39425543  781.58341     774.15961217 ...  742.79364602\n",
      "    740.56710028  742.09095043]\n",
      "  [ 781.32832257  773.5525543   771.4621248  ...  745.58948021\n",
      "    743.6724366   744.94973531]\n",
      "  [ 775.02880434  770.05940988  769.62341905 ...  747.58546843\n",
      "    744.7427733   745.49885435]\n",
      "  ...\n",
      "  [ 359.42812672  283.97752872  188.3704522  ...   43.52159775\n",
      "     90.44692584  194.91423326]\n",
      "  [ 258.43515373  177.66286532  171.61367139 ...   88.41635448\n",
      "     78.80607791  259.13568756]\n",
      "  [ 230.99011275  120.93421273  125.14762813 ...  117.07317999\n",
      "     83.78906968  268.4281339 ]]\n",
      "\n",
      " [[ 827.63565006  818.62986079  815.30196971 ...  778.37861394\n",
      "    774.86876508  781.77595443]\n",
      "  [ 816.46836304  810.41285417  812.62568825 ...  782.47188427\n",
      "    779.27364899  785.32687006]\n",
      "  [ 809.04644224  806.96639758  812.15469955 ...  784.09849333\n",
      "    780.66273992  786.61796456]\n",
      "  ...\n",
      "  [ 342.69308914  267.87720647  198.41205576 ...   42.54968429\n",
      "    136.63001128  256.57030246]\n",
      "  [ 247.82877414  176.56915626  156.9443292  ...   68.26638462\n",
      "    132.66522441  267.38973453]\n",
      "  [ 191.24051831   86.67384098   96.14287533 ...   97.90112885\n",
      "    139.3451388   257.74667864]]\n",
      "\n",
      " [[ 749.13399708  739.80246408  734.87183847 ...  706.0747036\n",
      "    703.31686078  707.82593363]\n",
      "  [ 737.56233713  731.35646545  731.48562408 ...  708.56413828\n",
      "    706.33474756  710.26096638]\n",
      "  [ 733.04620583  730.08378933  733.23643549 ...  708.41405507\n",
      "    705.42110672  709.25718815]\n",
      "  ...\n",
      "  [ 296.54100474  253.98135273  173.76053435 ...   38.28682461\n",
      "    107.70751741  209.11512422]\n",
      "  [ 261.58421541  154.95131978  113.79953427 ...   73.48760626\n",
      "     90.14131408  238.7148463 ]\n",
      "  [ 160.12266163   64.51092635  113.35695524 ...  117.08868775\n",
      "    116.28738754  283.98343987]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1087.58212018 1073.17867805 1065.31716106 ... 1021.8699219\n",
      "   1017.43709938 1023.78059765]\n",
      "  [1071.00287651 1060.84484101 1060.4932824  ... 1026.96699353\n",
      "   1022.44370814 1027.7609564 ]\n",
      "  [1062.76694438 1057.87641808 1061.44255027 ... 1027.38645539\n",
      "   1022.79365353 1027.64093157]\n",
      "  ...\n",
      "  [ 460.46955835  369.41654333  263.84518301 ...   50.33142231\n",
      "    174.78292843  271.03737037]\n",
      "  [ 387.66603111  257.90154922  158.69688348 ...   87.19771354\n",
      "    151.31245826  309.75039754]\n",
      "  [ 257.29409402  110.32047789  132.19513497 ...  151.9484513\n",
      "    163.89174887  366.3704823 ]]\n",
      "\n",
      " [[ 951.54865093  940.0288642   932.73516471 ...  890.80570497\n",
      "    888.52832339  893.7602575 ]\n",
      "  [ 938.12638572  931.95192254  929.49284582 ...  894.76032703\n",
      "    892.61662383  898.15863243]\n",
      "  [ 930.1039901   926.37031654  927.99371956 ...  897.88264875\n",
      "    895.22192771  900.17423889]\n",
      "  ...\n",
      "  [ 414.44517014  323.71274485  234.20342094 ...   57.53409361\n",
      "    141.17240817  269.24119053]\n",
      "  [ 280.04550724  214.06943107  226.38546603 ...  104.20429174\n",
      "    120.42385428  335.60803407]\n",
      "  [ 268.29708531  138.72189078  144.86232233 ...  132.10774395\n",
      "    110.20386429  330.73871645]]\n",
      "\n",
      " [[ 758.64215696  747.53175608  741.67871184 ...  713.96076591\n",
      "    710.26830199  711.04002312]\n",
      "  [ 747.55654844  739.16063186  737.87121965 ...  716.57733719\n",
      "    712.27306955  713.4451859 ]\n",
      "  [ 743.50020565  737.54158002  738.52700208 ...  716.85169379\n",
      "    713.62665769  713.71821855]\n",
      "  ...\n",
      "  [ 332.07541246  256.53408721  203.42904998 ...   33.23752936\n",
      "     87.08811243  146.50050925]\n",
      "  [ 282.35681055  173.58879478  111.71858081 ...   65.21232985\n",
      "     95.20802518  186.32272572]\n",
      "  [ 198.88512801  100.73884096   75.7041767  ...   90.73869033\n",
      "    106.99019589  204.73476753]]]\n",
      "\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Detector result: \n",
      "[[[ 792.39425543  781.58341     774.15961217 ...  742.79364602\n",
      "    740.56710028  742.09095043]\n",
      "  [ 781.32832257  773.5525543   771.4621248  ...  745.58948021\n",
      "    743.6724366   744.94973531]\n",
      "  [ 775.02880434  770.05940988  769.62341905 ...  747.58546843\n",
      "    744.7427733   745.49885435]\n",
      "  ...\n",
      "  [ 359.42812672  283.97752872  188.3704522  ...   43.52159775\n",
      "     90.44692584  194.91423326]\n",
      "  [ 258.43515373  177.66286532  171.61367139 ...   88.41635448\n",
      "     78.80607791  259.13568756]\n",
      "  [ 230.99011275  120.93421273  125.14762813 ...  117.07317999\n",
      "     83.78906968  268.4281339 ]]\n",
      "\n",
      " [[ 827.63565006  818.62986079  815.30196971 ...  778.37861394\n",
      "    774.86876508  781.77595443]\n",
      "  [ 816.46836304  810.41285417  812.62568825 ...  782.47188427\n",
      "    779.27364899  785.32687006]\n",
      "  [ 809.04644224  806.96639758  812.15469955 ...  784.09849333\n",
      "    780.66273992  786.61796456]\n",
      "  ...\n",
      "  [ 342.69308914  267.87720647  198.41205576 ...   42.54968429\n",
      "    136.63001128  256.57030246]\n",
      "  [ 247.82877414  176.56915626  156.9443292  ...   68.26638462\n",
      "    132.66522441  267.38973453]\n",
      "  [ 191.24051831   86.67384098   96.14287533 ...   97.90112885\n",
      "    139.3451388   257.74667864]]\n",
      "\n",
      " [[ 749.13399708  739.80246408  734.87183847 ...  706.0747036\n",
      "    703.31686078  707.82593363]\n",
      "  [ 737.56233713  731.35646545  731.48562408 ...  708.56413828\n",
      "    706.33474756  710.26096638]\n",
      "  [ 733.04620583  730.08378933  733.23643549 ...  708.41405507\n",
      "    705.42110672  709.25718815]\n",
      "  ...\n",
      "  [ 296.54100474  253.98135273  173.76053435 ...   38.28682461\n",
      "    107.70751741  209.11512422]\n",
      "  [ 261.58421541  154.95131978  113.79953427 ...   73.48760626\n",
      "     90.14131408  238.7148463 ]\n",
      "  [ 160.12266163   64.51092635  113.35695524 ...  117.08868775\n",
      "    116.28738754  283.98343987]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1087.58212018 1073.17867805 1065.31716106 ... 1021.8699219\n",
      "   1017.43709938 1023.78059765]\n",
      "  [1071.00287651 1060.84484101 1060.4932824  ... 1026.96699353\n",
      "   1022.44370814 1027.7609564 ]\n",
      "  [1062.76694438 1057.87641808 1061.44255027 ... 1027.38645539\n",
      "   1022.79365353 1027.64093157]\n",
      "  ...\n",
      "  [ 460.46955835  369.41654333  263.84518301 ...   50.33142231\n",
      "    174.78292843  271.03737037]\n",
      "  [ 387.66603111  257.90154922  158.69688348 ...   87.19771354\n",
      "    151.31245826  309.75039754]\n",
      "  [ 257.29409402  110.32047789  132.19513497 ...  151.9484513\n",
      "    163.89174887  366.3704823 ]]\n",
      "\n",
      " [[ 951.54865093  940.0288642   932.73516471 ...  890.80570497\n",
      "    888.52832339  893.7602575 ]\n",
      "  [ 938.12638572  931.95192254  929.49284582 ...  894.76032703\n",
      "    892.61662383  898.15863243]\n",
      "  [ 930.1039901   926.37031654  927.99371956 ...  897.88264875\n",
      "    895.22192771  900.17423889]\n",
      "  ...\n",
      "  [ 414.44517014  323.71274485  234.20342094 ...   57.53409361\n",
      "    141.17240817  269.24119053]\n",
      "  [ 280.04550724  214.06943107  226.38546603 ...  104.20429174\n",
      "    120.42385428  335.60803407]\n",
      "  [ 268.29708531  138.72189078  144.86232233 ...  132.10774395\n",
      "    110.20386429  330.73871645]]\n",
      "\n",
      " [[ 758.64215696  747.53175608  741.67871184 ...  713.96076591\n",
      "    710.26830199  711.04002312]\n",
      "  [ 747.55654844  739.16063186  737.87121965 ...  716.57733719\n",
      "    712.27306955  713.4451859 ]\n",
      "  [ 743.50020565  737.54158002  738.52700208 ...  716.85169379\n",
      "    713.62665769  713.71821855]\n",
      "  ...\n",
      "  [ 332.07541246  256.53408721  203.42904998 ...   33.23752936\n",
      "     87.08811243  146.50050925]\n",
      "  [ 282.35681055  173.58879478  111.71858081 ...   65.21232985\n",
      "     95.20802518  186.32272572]\n",
      "  [ 198.88512801  100.73884096   75.7041767  ...   90.73869033\n",
      "    106.99019589  204.73476753]]]\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "\n",
      "Pooling result: \n",
      "[[[ 776.57687917  772.14779502  773.47227275 ...  748.16744212\n",
      "    746.12315358  744.9387441 ]\n",
      "  [ 773.16576248  774.57712253  776.48638164 ...  755.40229524\n",
      "    752.36689386  749.07615922]\n",
      "  [ 772.1274695   773.64983801  772.32967335 ...  759.03488191\n",
      "    755.49643733  750.33924264]\n",
      "  ...\n",
      "  [ 251.35139688  276.63886246  332.13543687 ...  214.13105499\n",
      "    157.51739325  122.47741755]\n",
      "  [ 349.02841513  291.98339921  249.03531177 ...  272.33642954\n",
      "    171.46487507   93.45896187]\n",
      "  [ 276.87878792  212.30758692  200.1837701  ...  221.2725343\n",
      "    163.56297417   79.01427845]]\n",
      "\n",
      " [[ 814.36021393  812.27266973  813.58245823 ...  785.23081497\n",
      "    784.97100822  782.26029697]\n",
      "  [ 811.63127827  815.66731985  817.33953651 ...  793.03177004\n",
      "    790.69912204  785.98546507]\n",
      "  [ 811.73697152  814.76113658  812.25721254 ...  797.53652413\n",
      "    794.34244955  787.43264166]\n",
      "  ...\n",
      "  [ 252.97288853  298.48956991  367.5521664  ...  223.07535584\n",
      "    148.97400439  144.71708761]\n",
      "  [ 377.96072935  285.83216917  277.00363557 ...  307.68176206\n",
      "    144.75421651  106.8340115 ]\n",
      "  [ 279.80676516  217.73515715  215.74076113 ...  261.64828464\n",
      "    144.33617506   80.12054105]]\n",
      "\n",
      " [[ 735.61990633  733.88923668  736.11300862 ...  711.4025134\n",
      "    710.06769188  707.92067444]\n",
      "  [ 734.67075696  737.3368951   738.60728513 ...  717.83341209\n",
      "    715.54943256  711.50273505]\n",
      "  [ 733.72011667  735.99350947  734.04189195 ...  721.41813991\n",
      "    718.18424052  712.40839245]\n",
      "  ...\n",
      "  [ 234.8827869   283.53287539  327.03993843 ...  212.79017482\n",
      "    143.67467257  120.00079798]\n",
      "  [ 349.64729149  266.26667532  236.49735806 ...  270.89150074\n",
      "    146.86330263   92.4800551 ]\n",
      "  [ 247.17860696  185.17456309  192.45288738 ...  219.34728718\n",
      "    135.85204946   73.14070829]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1066.72276355 1062.81794662 1065.79402905 ... 1029.70058416\n",
      "   1027.88749779 1025.31158998]\n",
      "  [1063.93985013 1067.99605047 1070.3660921  ... 1039.44099144\n",
      "   1036.15154564 1030.74997964]\n",
      "  [1063.2528094  1066.47570871 1063.82884258 ... 1045.46473387\n",
      "   1040.74019773 1032.56815138]\n",
      "  ...\n",
      "  [ 330.52524207  404.75677459  475.37393336 ...  303.65535119\n",
      "    215.92348659  174.65115277]\n",
      "  [ 504.20967184  391.57572064  354.3651971  ...  388.94690309\n",
      "    220.35071743  128.91696897]\n",
      "  [ 375.69599662  272.6546938   285.5411861  ...  319.89446737\n",
      "    203.18881774  107.65707501]]\n",
      "\n",
      " [[ 934.26131779  929.29795386  930.52082329 ...  898.49043643\n",
      "    897.46736935  894.94627591]\n",
      "  [ 929.69795998  932.19178854  934.45656834 ...  907.63057585\n",
      "    904.5943597   899.95564431]\n",
      "  [ 929.44536595  931.35053418  929.6347741  ...  912.44758569\n",
      "    908.95269962  901.61413737]\n",
      "  ...\n",
      "  [ 293.38432578  326.58549953  412.21864943 ...  243.42584923\n",
      "    171.43662441  159.42271374]\n",
      "  [ 414.39554285  340.51885947  311.38173678 ...  333.94040288\n",
      "    176.73081387  123.33604614]\n",
      "  [ 330.93976572  264.40266502  237.45180581 ...  282.51781708\n",
      "    175.56745354   97.78687623]]\n",
      "\n",
      " [[ 743.55664584  740.63576232  742.37568482 ...  718.42635121\n",
      "    715.95009342  714.85966832]\n",
      "  [ 741.43343574  743.11108176  744.75114968 ...  725.08999391\n",
      "    722.10275339  718.84279535]\n",
      "  [ 739.99537475  741.86206236  740.38174416 ...  728.3620233\n",
      "    724.84705786  720.02147911]\n",
      "  ...\n",
      "  [ 245.10361213  277.33323599  314.8224601  ...  222.11151907\n",
      "    162.53144683  113.07738711]\n",
      "  [ 345.87452555  280.3472468   235.73292523 ...  271.98861626\n",
      "    179.12802368   78.00318239]\n",
      "  [ 261.60319876  193.51781652  198.56291299 ...  210.04787436\n",
      "    167.18754064   70.23898644]]]\n",
      "\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Flatten result: \n",
      "[776.57687917 772.14779502 773.47227275 ... 210.04787436 167.18754064\n",
      "  70.23898644]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[     0.         247467.1528873   62932.32369586  77320.76031805\n",
      " 706765.01956177 621669.72766634 177256.79669751 528104.24253481]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[0.]\n",
      "\n",
      "Forward propagation result: \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path)\n",
    "image_number = 0\n",
    "image_matrix = [image_matrix[image_number]]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.forward_propagate(image_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2c760ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil pooling forward\n",
      "[[[ 6.  8.]\n",
      "  [14. 16.]]]\n",
      "hasil pooling backward\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 2. 0. 1.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 1. 0. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "#max pooling testing\n",
    "input_data = np.array([[[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12],\n",
    "                       [13, 14, 15, 16]]])\n",
    "\n",
    "model = Model()\n",
    "polingLayer = model.PoolingLayer(2,2)\n",
    "pol_result_f = polingLayer.pool(input_data)\n",
    "print(\"hasil pooling forward\")\n",
    "print(pol_result_f)\n",
    "\n",
    "print(\"hasil pooling backward\")\n",
    "error_data = np.array([[[2, 1],\n",
    "                        [1, 2]]])\n",
    "\n",
    "print(polingLayer.backward(error_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54964277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobot awal : \n",
      "[[0.60927757 0.37104502 0.20923263]]\n",
      "hasil forward1 : \n",
      "[0.9999924  0.60985209 0.29219015]\n",
      "hasil forward2 : \n",
      "[0.89669124]\n",
      "hasil backward : \n",
      "hasil dE : \n",
      "[0.00609278 0.00371045 0.00209233]\n",
      "bobot update : \n",
      "[[0.05992776 0.03649465 0.02063107]]\n"
     ]
    }
   ],
   "source": [
    "#Dense layer testing\n",
    "denseLayer1 = Model().DenseLayer(unit_count=3, activation=\"sigmoid\")\n",
    "denseLayer = Model().DenseLayer(unit_count=1, activation=\"relu\")\n",
    "flatLayer = Model().FlattenLayer()\n",
    "flat_f = flatLayer.flatten(pol_result_f)\n",
    "dense_f1 = denseLayer1.dense(flat_f)\n",
    "dense_f = denseLayer.dense(dense_f1)\n",
    "\n",
    "print(\"bobot awal : \")\n",
    "print(denseLayer._weight)\n",
    "\n",
    "print(\"hasil forward1 : \")\n",
    "print(dense_f1)\n",
    "\n",
    "print(\"hasil forward2 : \")\n",
    "print(dense_f)\n",
    "\n",
    "\n",
    "print(\"hasil backward : \")\n",
    "\n",
    "# error_data = np.array([0.1, -0.2, 0.3])\n",
    "error_data = np.array([0.01])\n",
    "# Backward propagation\n",
    "dE = denseLayer.backward(error_data)\n",
    "print(\"hasil dE : \")\n",
    "print(dE)\n",
    "\n",
    "#Update bobot lapisan\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "denseLayer.update_weight(learning_rate, momentum)\n",
    "print(\"bobot update : \")\n",
    "print(denseLayer._weight)\n",
    "# print(denseLayer.input)\n",
    "# print(denseLayer._deltaW)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
