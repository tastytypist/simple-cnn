{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional Neural Network\n",
    "\n",
    "# Simple CNN\n",
    "**Simple CNN** is a convolutional neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.241130300Z",
     "start_time": "2023-10-05T11:04:52.944944900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.253382100Z",
     "start_time": "2023-10-05T11:04:53.246131700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model. In\n",
    "    addition, this class is also used to save and load the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model_object: \"Model\", file_name: str = \"model.json\") -> None:\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json = jsonpickle.encode(model_object, indent=4)\n",
    "            file.write(json)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str = \"model.json\") -> \"Model\":\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"r\") as file:\n",
    "            json = file.read()\n",
    "            return jsonpickle.decode(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9981df315bc56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.315203200Z",
     "start_time": "2023-10-05T11:04:53.302381400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._result = []\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def forward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing forward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the backward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing backward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._biases = None\n",
    "\n",
    "        def _pad_weights(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "            padding_size: int,\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            self._weight_dimension = len(weights)\n",
    "\n",
    "            self._weight_height = (weight_height := len(weights[0])) + 2 * padding_size\n",
    "            self._weight_width = (weight_width := len(weights[0][0])) + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        weights[i][j - padding_size][k - padding_size]\n",
    "                        if padding_size <= j < weight_height + padding_size\n",
    "                        or padding_size <= k < weight_width + padding_size\n",
    "                        else 0.0\n",
    "                        for k in range(self._weight_width)\n",
    "                    ]\n",
    "                    for j in range(self._weight_height)\n",
    "                ]\n",
    "                for i in range(self._weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += np.sum(feature)\n",
    "            return feature_maps\n",
    "\n",
    "        def forward_propagate(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.convolute(weights)\n",
    "            print(\"Convolution result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "\n",
    "        @staticmethod\n",
    "        def detect(feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def forward_propagate(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.detect(feature)\n",
    "            print(\"Detector result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "\n",
    "        def average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self.average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self.max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.pool(input_matrix)\n",
    "            print(\"Pooling result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._weight = np.random.randn(unit_count)\n",
    "            self._deltaW = np.zeros((unit_count))\n",
    "\n",
    "        def dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                self.ouput =  expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                self.ouput = np.maximum(result, 0)\n",
    "            return self.ouput\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.dense(input_matrix)\n",
    "            print(\"Dense result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            derivative_value = np.array([])\n",
    "            for i in range(self.output):\n",
    "                derivative_value = np.append(derivative_value, self.derivative_act_func(self._activation, i))\n",
    "            \n",
    "            self._deltaW += np.multiply(derivative_value, error)\n",
    "            dE = np.matmul(error, self._weight)\n",
    "            return dE\n",
    "        \n",
    "        def update_weight(self, learning_rate, momentum) -> None:\n",
    "            \"\"\" \n",
    "            Indicate and perform the update weight and bias on the model\n",
    "            Update weight formula = w  + momentum * w + learning_rate * errors * output\n",
    "            Update bias formula = bias + momentum * bias + learning_rate * errors\n",
    "            \"\"\"\n",
    "            for i in range(self._unit_count):\n",
    "                self._weight[i] = self._weight[i] - ((momentum * self._weight[i]) + (learning_rate * self._deltaW[i] * self.input))\n",
    "\n",
    "            self._bias = self._bias - ((momentum * self._bias) + (learning_rate * self._deltaW))\n",
    "            self._deltaW = np.zeros((self._unit_count))\n",
    "\n",
    "        def derivative_act_func(self, activation, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from activation function and input\n",
    "            \"\"\"\n",
    "            if(activation == \"sigmoid\"):\n",
    "                return self.sigmoid_detivative(input)\n",
    "            else:\n",
    "                return self.relu_derivative(input)\n",
    "            \n",
    "        def sigmoid_detivative(self, input) -> float:\n",
    "            \"\"\" \n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            sigmoid = 1/(1+np.exp(-input))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        \n",
    "        def relu_derivative(self, input) -> float:\n",
    "            \"\"\" \n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            if(input >= 0):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "\n",
    "        @staticmethod\n",
    "        def flatten(input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.flatten(input_matrix)\n",
    "            print(\"Flatten result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.forward_propagate(tensor)\n",
    "        print(\"Forward propagation result: \")\n",
    "        print(tensor)\n",
    "        self._result = tensor\n",
    "\n",
    "    def backward_propagate(self) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, tensor: npt.NDArray[npt.NDArray], epochs: int = 3, learning_rate: float = 0.01) -> None:\n",
    "        \"\"\"\n",
    "        Fit and train the CNN model.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        :param epochs: An integer specifying the number of training epochs.\n",
    "        :param learning_rate: A float specifying the learning rate of the model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80fb427e45bb865f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:05:10.033906700Z",
     "start_time": "2023-10-05T11:04:53.312209300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on convolution layer...\n",
      "\n",
      "Convolution result: \n",
      "[[[ 518.00623867  508.50634669  504.70159231 ...  487.77704907\n",
      "    484.17426784  483.48092541]\n",
      "  [ 511.24694066  504.82296992  503.1682234  ...  488.21547317\n",
      "    484.99003603  484.58338992]\n",
      "  [ 508.53080276  502.96791226  503.55985291 ...  490.54979017\n",
      "    486.52522003  486.49967256]\n",
      "  ...\n",
      "  [ 270.14084207  198.83162194  110.52850671 ...   18.89156003\n",
      "     47.39773508  115.07108251]\n",
      "  [ 175.29199377  107.96770598   84.02170649 ...   48.95276385\n",
      "     54.49732937  139.06712246]\n",
      "  [ 141.64669208   77.43524397   62.40921933 ...   58.05035492\n",
      "     70.98258445  126.37645669]]\n",
      "\n",
      " [[ 627.34826602  621.06464208  618.96991105 ...  594.62775067\n",
      "    592.68395951  596.52093486]\n",
      "  [ 619.82939605  615.85721079  616.61729198 ...  595.80292911\n",
      "    593.63197716  596.4843313 ]\n",
      "  [ 617.40990408  615.07190876  618.17959486 ...  596.8930948\n",
      "    593.78026144  596.19487861]\n",
      "  ...\n",
      "  [ 215.08956487  156.49611652  141.14996625 ...   36.61646918\n",
      "     71.82367488  170.11875944]\n",
      "  [ 201.20008437  131.53000524  134.68613774 ...   67.55341624\n",
      "     97.49111951  219.28522362]\n",
      "  [ 146.44363087   61.58928841   55.76246957 ...   99.16822305\n",
      "     85.92757485  189.48461111]]\n",
      "\n",
      " [[ 825.30558092  813.73945924  806.81665044 ...  775.94717992\n",
      "    772.50261973  774.45887526]\n",
      "  [ 814.04110441  805.27062957  803.05506992 ...  778.21541698\n",
      "    775.55320009  776.19286897]\n",
      "  [ 808.5775593   802.51082369  803.49804484 ...  780.36059108\n",
      "    777.04720461  776.80628381]\n",
      "  ...\n",
      "  [ 358.87165463  268.40960705  208.45360803 ...   42.05562576\n",
      "     94.35909481  169.00442696]\n",
      "  [ 287.97948405  191.17433539  156.08059132 ...   84.87572604\n",
      "    101.6279149   238.88213609]\n",
      "  [ 231.2144017   119.1332137    92.61374766 ...  113.75728821\n",
      "     94.30573082  241.23155407]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1084.35755424 1070.97583812 1064.29556795 ... 1022.47421261\n",
      "   1018.49282167 1023.8473338 ]\n",
      "  [1071.30427579 1062.26652032 1062.29871494 ... 1025.29214152\n",
      "   1021.69185791 1025.55071182]\n",
      "  [1065.19598772 1059.48280137 1062.25496063 ... 1028.79119243\n",
      "   1025.02431023 1027.95428771]\n",
      "  ...\n",
      "  [ 479.43608286  341.03539072  228.95995016 ...   56.34804762\n",
      "    147.52693539  281.2335115 ]\n",
      "  [ 334.27478195  232.16768894  214.54095624 ...  117.19158979\n",
      "    155.31892587  367.06197988]\n",
      "  [ 270.87707551  132.87120698  125.02514633 ...  151.46171026\n",
      "    140.53214422  327.3459277 ]]\n",
      "\n",
      " [[ 839.79861852  830.35449614  823.41365881 ...  786.32724018\n",
      "    784.89799364  788.44084942]\n",
      "  [ 828.6343783   822.75946645  820.28806044 ...  790.15620872\n",
      "    788.2509257   792.66064567]\n",
      "  [ 821.49321322  817.58499455  818.60197388 ...  792.63185636\n",
      "    790.53142917  794.02834157]\n",
      "  ...\n",
      "  [ 376.85605822  297.04014678  189.77542985 ...   51.52871627\n",
      "    113.25059124  238.62289049]\n",
      "  [ 238.4410417   181.83564885  211.27611189 ...   95.02435766\n",
      "     92.42803437  306.45096451]\n",
      "  [ 248.35124751  134.09009843  149.6459101  ...  128.02714498\n",
      "     87.77380889  305.85929538]]\n",
      "\n",
      " [[ 510.748969    507.32962809  503.99930765 ...  482.79925596\n",
      "    483.57410452  488.09531035]\n",
      "  [ 505.64797187  504.83140691  503.97795734 ...  484.22779928\n",
      "    485.05387189  488.91714832]\n",
      "  [ 504.0731914   503.82118408  504.70975292 ...  486.76780812\n",
      "    487.07778842  489.82142494]\n",
      "  ...\n",
      "  [ 207.74345151  134.05178584  101.1155252  ...   37.26939554\n",
      "    106.62963122  133.16329165]\n",
      "  [ 136.34512126  103.68621645  107.24807442 ...   77.48781956\n",
      "     94.21517856  218.88410713]\n",
      "  [ 105.75974837   51.33885658   68.97551302 ...   89.10889786\n",
      "     55.39786386  200.59953602]]]\n",
      "\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Detector result: \n",
      "[[[ 518.00623867  508.50634669  504.70159231 ...  487.77704907\n",
      "    484.17426784  483.48092541]\n",
      "  [ 511.24694066  504.82296992  503.1682234  ...  488.21547317\n",
      "    484.99003603  484.58338992]\n",
      "  [ 508.53080276  502.96791226  503.55985291 ...  490.54979017\n",
      "    486.52522003  486.49967256]\n",
      "  ...\n",
      "  [ 270.14084207  198.83162194  110.52850671 ...   18.89156003\n",
      "     47.39773508  115.07108251]\n",
      "  [ 175.29199377  107.96770598   84.02170649 ...   48.95276385\n",
      "     54.49732937  139.06712246]\n",
      "  [ 141.64669208   77.43524397   62.40921933 ...   58.05035492\n",
      "     70.98258445  126.37645669]]\n",
      "\n",
      " [[ 627.34826602  621.06464208  618.96991105 ...  594.62775067\n",
      "    592.68395951  596.52093486]\n",
      "  [ 619.82939605  615.85721079  616.61729198 ...  595.80292911\n",
      "    593.63197716  596.4843313 ]\n",
      "  [ 617.40990408  615.07190876  618.17959486 ...  596.8930948\n",
      "    593.78026144  596.19487861]\n",
      "  ...\n",
      "  [ 215.08956487  156.49611652  141.14996625 ...   36.61646918\n",
      "     71.82367488  170.11875944]\n",
      "  [ 201.20008437  131.53000524  134.68613774 ...   67.55341624\n",
      "     97.49111951  219.28522362]\n",
      "  [ 146.44363087   61.58928841   55.76246957 ...   99.16822305\n",
      "     85.92757485  189.48461111]]\n",
      "\n",
      " [[ 825.30558092  813.73945924  806.81665044 ...  775.94717992\n",
      "    772.50261973  774.45887526]\n",
      "  [ 814.04110441  805.27062957  803.05506992 ...  778.21541698\n",
      "    775.55320009  776.19286897]\n",
      "  [ 808.5775593   802.51082369  803.49804484 ...  780.36059108\n",
      "    777.04720461  776.80628381]\n",
      "  ...\n",
      "  [ 358.87165463  268.40960705  208.45360803 ...   42.05562576\n",
      "     94.35909481  169.00442696]\n",
      "  [ 287.97948405  191.17433539  156.08059132 ...   84.87572604\n",
      "    101.6279149   238.88213609]\n",
      "  [ 231.2144017   119.1332137    92.61374766 ...  113.75728821\n",
      "     94.30573082  241.23155407]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1084.35755424 1070.97583812 1064.29556795 ... 1022.47421261\n",
      "   1018.49282167 1023.8473338 ]\n",
      "  [1071.30427579 1062.26652032 1062.29871494 ... 1025.29214152\n",
      "   1021.69185791 1025.55071182]\n",
      "  [1065.19598772 1059.48280137 1062.25496063 ... 1028.79119243\n",
      "   1025.02431023 1027.95428771]\n",
      "  ...\n",
      "  [ 479.43608286  341.03539072  228.95995016 ...   56.34804762\n",
      "    147.52693539  281.2335115 ]\n",
      "  [ 334.27478195  232.16768894  214.54095624 ...  117.19158979\n",
      "    155.31892587  367.06197988]\n",
      "  [ 270.87707551  132.87120698  125.02514633 ...  151.46171026\n",
      "    140.53214422  327.3459277 ]]\n",
      "\n",
      " [[ 839.79861852  830.35449614  823.41365881 ...  786.32724018\n",
      "    784.89799364  788.44084942]\n",
      "  [ 828.6343783   822.75946645  820.28806044 ...  790.15620872\n",
      "    788.2509257   792.66064567]\n",
      "  [ 821.49321322  817.58499455  818.60197388 ...  792.63185636\n",
      "    790.53142917  794.02834157]\n",
      "  ...\n",
      "  [ 376.85605822  297.04014678  189.77542985 ...   51.52871627\n",
      "    113.25059124  238.62289049]\n",
      "  [ 238.4410417   181.83564885  211.27611189 ...   95.02435766\n",
      "     92.42803437  306.45096451]\n",
      "  [ 248.35124751  134.09009843  149.6459101  ...  128.02714498\n",
      "     87.77380889  305.85929538]]\n",
      "\n",
      " [[ 510.748969    507.32962809  503.99930765 ...  482.79925596\n",
      "    483.57410452  488.09531035]\n",
      "  [ 505.64797187  504.83140691  503.97795734 ...  484.22779928\n",
      "    485.05387189  488.91714832]\n",
      "  [ 504.0731914   503.82118408  504.70975292 ...  486.76780812\n",
      "    487.07778842  489.82142494]\n",
      "  ...\n",
      "  [ 207.74345151  134.05178584  101.1155252  ...   37.26939554\n",
      "    106.62963122  133.16329165]\n",
      "  [ 136.34512126  103.68621645  107.24807442 ...   77.48781956\n",
      "     94.21517856  218.88410713]\n",
      "  [ 105.75974837   51.33885658   68.97551302 ...   89.10889786\n",
      "     55.39786386  200.59953602]]]\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "\n",
      "Pooling result: \n",
      "[[[ 507.27898662  504.86109097  505.641094   ...  490.03302507\n",
      "    488.14251913  487.71231409]\n",
      "  [ 505.63189891  506.6754582   507.63199013 ...  494.60597382\n",
      "    492.52499325  490.79014533]\n",
      "  [ 503.80958683  505.78771702  504.75624672 ...  496.50982665\n",
      "    494.08822214  491.24712586]\n",
      "  ...\n",
      "  [ 169.43938124  189.09796876  209.21658    ...  153.06400089\n",
      "    118.06772431   76.05361217]\n",
      "  [ 236.08569109  190.01778287  162.05147142 ...  178.00135117\n",
      "    126.94240003   52.42789499]\n",
      "  [ 181.62275963  129.12207362  141.09419048 ...  143.06187954\n",
      "    119.660961     48.63793901]]\n",
      "\n",
      " [[ 618.92756952  618.23876982  619.96576949 ...  598.90798201\n",
      "    597.78091898  595.9257677 ]\n",
      "  [ 618.31136494  620.43927643  620.9387621  ...  604.63349645\n",
      "    602.65502485  599.03318784]\n",
      "  [ 616.98289853  618.64291547  616.99991828 ...  606.74070488\n",
      "    604.36666844  599.17880963]\n",
      "  ...\n",
      "  [ 216.36881934  226.90394022  267.62140769 ...  189.14653132\n",
      "    116.93911234   97.73702278]\n",
      "  [ 285.20858067  219.34247555  192.03169693 ...  231.42400661\n",
      "    125.8922176    72.07856686]\n",
      "  [ 197.83493464  161.52174586  154.71975903 ...  182.13866181\n",
      "    122.43194421   59.66908615]]\n",
      "\n",
      " [[ 809.20165804  805.5225163   807.19266146 ...  781.00974247\n",
      "    778.62500164  777.56145947]\n",
      "  [ 806.45397943  808.15156294  809.67881392 ...  788.58304399\n",
      "    785.27477107  781.79953838]\n",
      "  [ 804.82861303  806.50872532  805.13677285 ...  791.71899823\n",
      "    788.08689511  782.86936068]\n",
      "  ...\n",
      "  [ 270.23563416  293.8366565   341.73572934 ...  236.29552447\n",
      "    170.98714855  122.67837008]\n",
      "  [ 367.4544267   304.40651987  255.90399248 ...  287.74061911\n",
      "    188.71747003   88.35797414]\n",
      "  [ 283.90114888  216.71472405  209.8610043  ...  225.93247848\n",
      "    178.32771188   80.5607307 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1066.93691345 1063.4260801  1065.97087747 ... 1030.58811827\n",
      "   1028.47507951 1025.686436  ]\n",
      "  [1064.27186516 1067.50343627 1069.15737338 ... 1040.19335688\n",
      "   1037.00999193 1031.45450584]\n",
      "  [1062.11863515 1065.45852071 1063.04279379 ... 1044.44585985\n",
      "   1040.18622652 1032.34341347]\n",
      "  ...\n",
      "  [ 357.36622212  393.91111213  458.41170845 ...  312.13994313\n",
      "    215.45095288  169.48269159]\n",
      "  [ 487.93053799  383.20622052  346.19539864 ...  377.3713606\n",
      "    221.56784827  125.5500888 ]\n",
      "  [ 362.42779879  280.02398802  279.10954425 ...  311.71194128\n",
      "    213.26524121  111.18011704]]\n",
      "\n",
      " [[ 824.76987337  820.22289776  821.02662855 ...  793.15659971\n",
      "    792.18395502  790.15038637]\n",
      "  [ 820.64413796  822.55382541  824.46889462 ...  801.42106253\n",
      "    798.26338116  794.41477428]\n",
      "  [ 820.42038456  821.66676928  820.38506146 ...  805.26956696\n",
      "    802.22019206  796.09161423]\n",
      "  ...\n",
      "  [ 263.44791809  284.67811674  360.48308653 ...  214.21245579\n",
      "    151.98405471  138.70463705]\n",
      "  [ 363.3637654   302.39194286  271.77482382 ...  290.90860981\n",
      "    160.43958642  108.73466577]\n",
      "  [ 291.93899137  235.95155698  208.18364802 ...  244.90566784\n",
      "    159.58802152   86.8451835 ]]\n",
      "\n",
      " [[ 505.45992992  504.1259773   505.86433118 ...  488.10365977\n",
      "    487.81090534  485.80591872]\n",
      "  [ 505.02036162  506.23294274  506.93736266 ...  492.94081872\n",
      "    491.89585479  488.47478306]\n",
      "  [ 504.00756555  504.7482828   503.80721608 ...  494.67605138\n",
      "    492.90494323  488.49249114]\n",
      "  ...\n",
      "  [ 176.20119876  192.16240538  220.95683962 ...  145.93261808\n",
      "     90.90288884   81.97946828]\n",
      "  [ 223.44037283  172.37168925  165.25144546 ...  168.40335025\n",
      "     84.02588932   66.86878169]\n",
      "  [ 158.35135542  136.80798997  123.01277945 ...  143.30054613\n",
      "     81.80273772   64.55797186]]]\n",
      "\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Flatten result: \n",
      "[507.27898662 504.86109097 505.641094   ... 143.30054613  81.80273772\n",
      "  64.55797186]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[3.03561621e+07 1.59344393e+08 0.00000000e+00 3.24473703e+07\n",
      " 2.54548242e+07 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[1.]\n",
      "\n",
      "Forward propagation result: \n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path)\n",
    "image_number = 0\n",
    "image_matrix = [image_matrix[image_number]]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.forward_propagate(image_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
