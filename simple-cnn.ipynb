{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional Neural Network\n",
    "\n",
    "# Simple CNN\n",
    "**Simple CNN** is a convolutional neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n",
    "\n",
    "## Contribution (Milestone 2)\n",
    "| NIM      | Name                   | Contribution(s) |\n",
    "|----------|------------------------|-----------------|\n",
    "| 13520041 | Ilham Pratama          | -               |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | -               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.241130300Z",
     "start_time": "2023-10-05T11:04:52.944944900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from scipy.special import expit\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.253382100Z",
     "start_time": "2023-10-05T11:04:53.246131700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model. In\n",
    "    addition, this class is also used to save and load the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model_object: \"Model\", file_name: str = \"model.json\") -> None:\n",
    "        \"\"\"\n",
    "        Save the specified model into a JSON file.\n",
    "\n",
    "        :param model_object: The model to be saved.\n",
    "        :param file_name: A string specifying the file name of the saved model.\n",
    "        \"\"\"\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json = jsonpickle.encode(model_object, indent=4)\n",
    "            file.write(json)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str = \"model.json\") -> \"Model\":\n",
    "        \"\"\"\n",
    "        Load a model from the specified JSON file name.\n",
    "\n",
    "        :param file_name: A string specifying the file name of the model to be\n",
    "                          loaded.\n",
    "        :return: The loaded model from the specified file.\n",
    "        \"\"\"\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"r\") as file:\n",
    "            json = file.read()\n",
    "            return jsonpickle.decode(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9981df315bc56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.315203200Z",
     "start_time": "2023-10-05T11:04:53.302381400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._results = []\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def forward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing forward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the backward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing backward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._weights = None\n",
    "            self._biases = None\n",
    "            self._filter_gradients = []\n",
    "            self._bias_gradients = []\n",
    "\n",
    "        def _pad_weights(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "            padding_size: int,\n",
    "            forward: bool = True\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param forward: A boolean specifying whether the padding is \n",
    "                            performed during forward propagation.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            weight_dimension = len(weights)\n",
    "            weight_height = len(weights[0])\n",
    "            weight_width = len(weights[0][0])\n",
    "            \n",
    "            if forward:\n",
    "                self._weight_dimension = weight_dimension\n",
    "                self._weight_height = weight_height + 2 * padding_size\n",
    "                self._weight_width = weight_width + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        weights[i][j - padding_size][k - padding_size]\n",
    "                        if padding_size <= j < weight_height + padding_size\n",
    "                        or padding_size <= k < weight_width + padding_size\n",
    "                        else 0.0\n",
    "                        for k in range(weight_width + 2 * padding_size)\n",
    "                    ]\n",
    "                    for j in range(weight_height + 2 * padding_size)\n",
    "                ]\n",
    "                for i in range(weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._weights = weights\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += np.sum(feature)\n",
    "            return feature_maps\n",
    "\n",
    "        def calculate_error(self, output_gradient: npt.NDArray) -> npt.NDArray:\n",
    "            output_gradient_height = len(output_gradient[0])\n",
    "            output_gradient_width = len(output_gradient[0][0])\n",
    "            \n",
    "            filter_gradient = np.zeros((self._filter_count, self._filter_dimension, self._filter_height, self._filter_width))\n",
    "            input_gradient = np.zeros((self._weight_dimension, self._weight_height, self._weight_width))\n",
    "            padded_output_gradient = self._pad_weights(output_gradient, 2, forward=False)\n",
    "\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(self._filter_dimension):\n",
    "                    for k in range(0, self._weight_height - output_gradient_height + 1, self._stride_height):\n",
    "                        for l in range(0, self._weight_width - output_gradient_width + 1, self._stride_width):\n",
    "                            field = self._weights[j, k : k + output_gradient_height, l : l + output_gradient_width]\n",
    "                            gradient = field * output_gradient[i]\n",
    "                            filter_gradient[i][j] = np.sum(gradient)\n",
    "                    for k in range(0, output_gradient_height - self._filter_height + 1, self._stride_height):\n",
    "                        for l in range(0, output_gradient_width - self._filter_width + 1, self._stride_width):\n",
    "                            field = padded_output_gradient[i, k : k + self._filter_height, l : l + self._filter_width]\n",
    "                            gradient = field * np.rot90(self._filter_weights[i][j], k=2)\n",
    "                            input_gradient[j] += np.sum(gradient)\n",
    "            \n",
    "            self._filter_gradients.append(filter_gradient)\n",
    "            self._bias_gradients.append(output_gradient)\n",
    "            \n",
    "            return input_gradient\n",
    "        \n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            self._filter_weights -= learning_rate * np.average(np.array(self._filter_gradients), axis=0)\n",
    "            self._biases -= learning_rate * np.average(np.array(self._bias_gradients), axis=0)\n",
    "            self._filter_gradients = []\n",
    "            self._bias_gradients = []\n",
    "\n",
    "        def forward_propagate(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.convolute(weights)\n",
    "            print(\"Convolution result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "\n",
    "        # @staticmethod\n",
    "        def detect(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            self.input = feature\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def forward_propagate(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.detect(feature)\n",
    "            print(\"Detector result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def backward(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the detector layer.\n",
    "            Using Relu derivative : dReLU/dx = 1 if x > 0, otherwise 0\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            dX = error * (self.input > 0)\n",
    "            return dX\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "\n",
    "        def average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self.average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self.max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.pool(input_matrix)\n",
    "            print(\"Pooling result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def backward(self, error):\n",
    "            f, w, h = self.input.shape\n",
    "            dx = np.zeros(self.input.shape)\n",
    "            for i in range(0, f):\n",
    "                for j in range(0, w, self._filter_size):\n",
    "                    for k in range(0, h, self._filter_size):\n",
    "                        input_slice = self.input[i, j : j + self._filter_size, k : k + self._filter_size]\n",
    "                        max_input_slice = np.argmax(input_slice)\n",
    "                        max_idx = np.unravel_index(max_input_slice, (self._filter_size, self._filter_size))\n",
    "                        if (j + max_idx[0]) < w and (k + max_idx[1]) < h:\n",
    "                            dx[i, j + max_idx[0], k + max_idx[1]] = error[\n",
    "                                i, int(j // self._filter_size), int(k // self._filter_size)\n",
    "                            ]\n",
    "            return dx\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._weight = []\n",
    "            self._deltaW = np.zeros(unit_count)\n",
    "\n",
    "        def dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            if len(self._weight) == 0:\n",
    "                self._weight = np.random.randn(self._unit_count, len(self.input))\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                self.output = expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                self.output = np.maximum(result, 0)\n",
    "            return self.output\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.dense(input_matrix)\n",
    "            print(\"Dense result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def backward(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the layer.\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            derivative_value = np.array([])\n",
    "            for i in self.output:\n",
    "                derivative_value = np.append(derivative_value, self.derivative_act_func(self._activation, i))\n",
    "\n",
    "            self._deltaW += np.multiply(derivative_value, error)\n",
    "            dE = np.matmul(error, self._weight)\n",
    "            return dE\n",
    "\n",
    "        def update_weight(self, learning_rate) -> None:\n",
    "            \"\"\"\n",
    "            Indicate and perform the update weight and bias on the model\n",
    "            \"\"\"\n",
    "            for i in range(self._unit_count):\n",
    "                self._weight[i] = self._weight[i] - (learning_rate * self._deltaW[i] * self.input)\n",
    "\n",
    "            self._bias = self._bias - (learning_rate * self._deltaW)\n",
    "            self._deltaW = np.zeros(self._unit_count)\n",
    "\n",
    "        def derivative_act_func(self, activation, input_):\n",
    "            \"\"\"\n",
    "            Take derivative value from activation function and input\n",
    "            \"\"\"\n",
    "            if activation == \"sigmoid\":\n",
    "                return self.sigmoid_detivative(input_)\n",
    "            else:\n",
    "                return self.relu_derivative(input_)\n",
    "\n",
    "        @staticmethod\n",
    "        def sigmoid_derivative(input_):\n",
    "            \"\"\"\n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            sigmoid = 1 / (1 + np.exp(-input_))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "\n",
    "        @staticmethod\n",
    "        def relu_derivative(input_):\n",
    "            \"\"\"\n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            if input_ >= 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "\n",
    "        @staticmethod\n",
    "        def flatten(input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.flatten(input_matrix)\n",
    "            print(\"Flatten result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.forward_propagate(tensor)\n",
    "        print(\"Forward propagation result: \")\n",
    "        print(tensor)\n",
    "        self._results = tensor\n",
    "\n",
    "    def backward_propagate(self) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        self._backward_result = None\n",
    "        pass\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        tensor: npt.NDArray[npt.NDArray],\n",
    "        target: npt.NDArray,\n",
    "        epochs: int = 1,\n",
    "        batch_size: int = 5,\n",
    "        learning_rate: float = 0.01,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Fit and train the CNN model.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        :param target: An ndarray of representations of the target pictures to\n",
    "                       be fed into the model.\n",
    "        :param epochs: An integer specifying the number of training epochs.\n",
    "        :param batch_size: An integer specifying the number of training batch.\n",
    "        :param learning_rate: A float specifying the learning rate of the model.\n",
    "        \"\"\"\n",
    "        out = np.array([])\n",
    "        y_target = np.array([])\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            print(\"Epoch : \", epoch + 1)\n",
    "            for i in range(len(tensor)):\n",
    "                self.forward_propagate(tensor[i])\n",
    "                forward_result = self._results\n",
    "                curr_target = target[i]\n",
    "                curr_output = forward_result[0]\n",
    "                dE = np.array([curr_target - curr_output]) * -1\n",
    "                self.backward_propagate(dE)\n",
    "                loss += 0.5 * (curr_target - curr_output) ** 2\n",
    "                out = np.rint(np.append(out, curr_output))\n",
    "                y_target = np.append(y_target, curr_target)\n",
    "\n",
    "                # ketika sudah mencapai akhir dari batch update weightnya\n",
    "                if (i + 1) % batch_size == 0:\n",
    "                    for layer in reversed(self._layers):\n",
    "                        layer.update_weight(learning_rate)\n",
    "\n",
    "            avg_loss = loss / len(tensor)\n",
    "            print(\"Loss: \", avg_loss)\n",
    "            print(\"Accuracy: \", metrics.accuracy_score(y_target, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80fb427e45bb865f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:05:10.033906700Z",
     "start_time": "2023-10-05T11:04:53.312209300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on convolution layer...\n",
      "\n",
      "Convolution result: \n",
      "[[[ 549.29001939  540.12761152  537.62317567 ...  520.26862839\n",
      "    515.92903654  516.91215601]\n",
      "  [ 541.35055545  535.46095942  535.16095583 ...  520.76202737\n",
      "    517.03218064  517.17131029]\n",
      "  [ 539.62599808  534.50748302  536.39326033 ...  521.40666455\n",
      "    517.59355828  517.23080858]\n",
      "  ...\n",
      "  [ 219.88770392  164.52454597  131.72043236 ...   23.0516546\n",
      "     45.89508213  122.64506879]\n",
      "  [ 202.43375699  114.8488342    83.77102647 ...   48.22645193\n",
      "     72.750218    146.23311602]\n",
      "  [ 129.03189531   57.76150697   41.05349154 ...   70.24052913\n",
      "     85.75162477  132.67993539]]\n",
      "\n",
      " [[ 839.91222871  828.52905545  824.71496353 ...  792.40231259\n",
      "    787.44444685  790.98079594]\n",
      "  [ 829.50454303  821.99714408  822.04236944 ...  794.40338993\n",
      "    789.33881196  792.60131227]\n",
      "  [ 824.2717663   819.49842501  821.73678084 ...  796.46853097\n",
      "    791.92797621  793.46418736]\n",
      "  ...\n",
      "  [ 371.5234435   270.97630993  184.09054619 ...   41.4853642\n",
      "     83.32764242  209.87077048]\n",
      "  [ 236.13357888  147.41183392  167.60988331 ...   75.40778427\n",
      "    117.57927713  249.5245588 ]\n",
      "  [ 215.37114631  126.35360684  103.8708964  ...   98.30812267\n",
      "    130.78832797  208.19158505]]\n",
      "\n",
      " [[1003.26806653  989.77405875  982.4228995  ...  946.5091283\n",
      "    942.53767705  943.90312565]\n",
      "  [ 990.79887732  980.85396087  978.85055154 ...  947.96483884\n",
      "    943.8636414   945.92913423]\n",
      "  [ 986.06570702  978.50115047  979.6559873  ...  950.24551573\n",
      "    946.53750686  947.29637875]\n",
      "  ...\n",
      "  [ 441.19630546  321.63652761  230.62348674 ...   52.7130789\n",
      "    112.22364722  213.12809121]\n",
      "  [ 325.43816924  206.03851128  187.52318864 ...  111.33742982\n",
      "    127.76976997  308.87056014]\n",
      "  [ 262.97274698  142.12337459  123.18699009 ...  140.87105999\n",
      "    121.80561862  293.69602137]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 801.49508921  789.90563186  783.0078654  ...  752.92908752\n",
      "    747.87431834  747.99546889]\n",
      "  [ 789.93073703  780.27321147  778.44140183 ...  754.91313656\n",
      "    750.80248854  750.66636751]\n",
      "  [ 783.99636878  776.59732804  778.24670477 ...  756.24307851\n",
      "    752.08995402  751.06643713]\n",
      "  ...\n",
      "  [ 364.74420917  280.86795332  196.66101868 ...   34.2389078\n",
      "     73.38668906  154.12586742]\n",
      "  [ 269.57755719  177.74724304  145.65463669 ...   51.29941848\n",
      "     93.82861295  185.8637954 ]\n",
      "  [ 242.75345598  140.00825635  100.58665468 ...  100.74854058\n",
      "    108.52401114  200.22193352]]\n",
      "\n",
      " [[ 847.42543576  833.51109607  828.15094372 ...  796.99983498\n",
      "    791.3172657   792.33530671]\n",
      "  [ 835.11865718  824.75121311  823.8190028  ...  799.95368473\n",
      "    794.26646943  794.77484025]\n",
      "  [ 829.04107615  821.09094103  824.26456904 ...  800.57708637\n",
      "    795.1415908   795.76678904]\n",
      "  ...\n",
      "  [ 374.28141737  285.82111834  217.21692793 ...   31.16152714\n",
      "     89.69940496  189.18950576]\n",
      "  [ 313.70334574  209.05234481  137.88174115 ...   52.95389437\n",
      "    110.82753293  199.38877879]\n",
      "  [ 235.65774272  111.7469677    65.30824819 ...   93.74356165\n",
      "    127.74594482  195.96336123]]\n",
      "\n",
      " [[ 794.24913392  785.0814742   782.42928464 ...  750.15040435\n",
      "    747.03140324  751.88807882]\n",
      "  [ 783.91650192  778.66029468  779.51104473 ...  752.68768994\n",
      "    748.88934864  753.40272463]\n",
      "  [ 780.08885148  776.74479239  779.98401946 ...  753.97677425\n",
      "    750.08057788  754.11546407]\n",
      "  ...\n",
      "  [ 303.36839751  225.80886947  176.84937744 ...   44.10927111\n",
      "     92.69535183  224.78262268]\n",
      "  [ 236.67897655  157.19832708  172.71552098 ...   79.40492889\n",
      "    117.3434893   270.75124715]\n",
      "  [ 193.01993862   92.4903053    88.35009192 ...  113.0788595\n",
      "    115.5617496   233.85313243]]]\n",
      "\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Detector result: \n",
      "[[[ 549.29001939  540.12761152  537.62317567 ...  520.26862839\n",
      "    515.92903654  516.91215601]\n",
      "  [ 541.35055545  535.46095942  535.16095583 ...  520.76202737\n",
      "    517.03218064  517.17131029]\n",
      "  [ 539.62599808  534.50748302  536.39326033 ...  521.40666455\n",
      "    517.59355828  517.23080858]\n",
      "  ...\n",
      "  [ 219.88770392  164.52454597  131.72043236 ...   23.0516546\n",
      "     45.89508213  122.64506879]\n",
      "  [ 202.43375699  114.8488342    83.77102647 ...   48.22645193\n",
      "     72.750218    146.23311602]\n",
      "  [ 129.03189531   57.76150697   41.05349154 ...   70.24052913\n",
      "     85.75162477  132.67993539]]\n",
      "\n",
      " [[ 839.91222871  828.52905545  824.71496353 ...  792.40231259\n",
      "    787.44444685  790.98079594]\n",
      "  [ 829.50454303  821.99714408  822.04236944 ...  794.40338993\n",
      "    789.33881196  792.60131227]\n",
      "  [ 824.2717663   819.49842501  821.73678084 ...  796.46853097\n",
      "    791.92797621  793.46418736]\n",
      "  ...\n",
      "  [ 371.5234435   270.97630993  184.09054619 ...   41.4853642\n",
      "     83.32764242  209.87077048]\n",
      "  [ 236.13357888  147.41183392  167.60988331 ...   75.40778427\n",
      "    117.57927713  249.5245588 ]\n",
      "  [ 215.37114631  126.35360684  103.8708964  ...   98.30812267\n",
      "    130.78832797  208.19158505]]\n",
      "\n",
      " [[1003.26806653  989.77405875  982.4228995  ...  946.5091283\n",
      "    942.53767705  943.90312565]\n",
      "  [ 990.79887732  980.85396087  978.85055154 ...  947.96483884\n",
      "    943.8636414   945.92913423]\n",
      "  [ 986.06570702  978.50115047  979.6559873  ...  950.24551573\n",
      "    946.53750686  947.29637875]\n",
      "  ...\n",
      "  [ 441.19630546  321.63652761  230.62348674 ...   52.7130789\n",
      "    112.22364722  213.12809121]\n",
      "  [ 325.43816924  206.03851128  187.52318864 ...  111.33742982\n",
      "    127.76976997  308.87056014]\n",
      "  [ 262.97274698  142.12337459  123.18699009 ...  140.87105999\n",
      "    121.80561862  293.69602137]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 801.49508921  789.90563186  783.0078654  ...  752.92908752\n",
      "    747.87431834  747.99546889]\n",
      "  [ 789.93073703  780.27321147  778.44140183 ...  754.91313656\n",
      "    750.80248854  750.66636751]\n",
      "  [ 783.99636878  776.59732804  778.24670477 ...  756.24307851\n",
      "    752.08995402  751.06643713]\n",
      "  ...\n",
      "  [ 364.74420917  280.86795332  196.66101868 ...   34.2389078\n",
      "     73.38668906  154.12586742]\n",
      "  [ 269.57755719  177.74724304  145.65463669 ...   51.29941848\n",
      "     93.82861295  185.8637954 ]\n",
      "  [ 242.75345598  140.00825635  100.58665468 ...  100.74854058\n",
      "    108.52401114  200.22193352]]\n",
      "\n",
      " [[ 847.42543576  833.51109607  828.15094372 ...  796.99983498\n",
      "    791.3172657   792.33530671]\n",
      "  [ 835.11865718  824.75121311  823.8190028  ...  799.95368473\n",
      "    794.26646943  794.77484025]\n",
      "  [ 829.04107615  821.09094103  824.26456904 ...  800.57708637\n",
      "    795.1415908   795.76678904]\n",
      "  ...\n",
      "  [ 374.28141737  285.82111834  217.21692793 ...   31.16152714\n",
      "     89.69940496  189.18950576]\n",
      "  [ 313.70334574  209.05234481  137.88174115 ...   52.95389437\n",
      "    110.82753293  199.38877879]\n",
      "  [ 235.65774272  111.7469677    65.30824819 ...   93.74356165\n",
      "    127.74594482  195.96336123]]\n",
      "\n",
      " [[ 794.24913392  785.0814742   782.42928464 ...  750.15040435\n",
      "    747.03140324  751.88807882]\n",
      "  [ 783.91650192  778.66029468  779.51104473 ...  752.68768994\n",
      "    748.88934864  753.40272463]\n",
      "  [ 780.08885148  776.74479239  779.98401946 ...  753.97677425\n",
      "    750.08057788  754.11546407]\n",
      "  ...\n",
      "  [ 303.36839751  225.80886947  176.84937744 ...   44.10927111\n",
      "     92.69535183  224.78262268]\n",
      "  [ 236.67897655  157.19832708  172.71552098 ...   79.40492889\n",
      "    117.3434893   270.75124715]\n",
      "  [ 193.01993862   92.4903053    88.35009192 ...  113.0788595\n",
      "    115.5617496   233.85313243]]]\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "\n",
      "Pooling result: \n",
      "[[[538.83777986 537.83819278 539.4507759  ... 522.24677549 520.25675399\n",
      "   519.61585349]\n",
      "  [538.37758373 540.04759514 540.63772258 ... 526.91754317 524.84259824\n",
      "   522.50308664]\n",
      "  [536.61229065 538.78175094 537.12348527 ... 528.66371968 526.15180411\n",
      "   522.61351993]\n",
      "  ...\n",
      "  [187.07697036 204.61612209 225.94782519 ... 172.93789588 118.32667297\n",
      "    79.8123809 ]\n",
      "  [256.05508324 198.21820145 164.89316595 ... 202.32498849 130.96475733\n",
      "    54.10382538]\n",
      "  [179.5643767  132.85501312 143.76907136 ... 153.8780992  122.89473478\n",
      "    47.28547102]]\n",
      "\n",
      " [[825.80080849 823.53510554 824.84132528 ... 797.55828654 795.64676876\n",
      "   793.81950135]\n",
      "  [823.28972952 825.92365923 826.79711508 ... 805.59644243 802.27829209\n",
      "   798.17096659]\n",
      "  [821.76081176 824.44837551 821.98536158 ... 808.15077948 804.93031279\n",
      "   799.00537368]\n",
      "  ...\n",
      "  [288.91823849 294.86819353 347.2694313  ... 244.63892508 163.98871992\n",
      "   133.01230834]\n",
      "  [374.13160784 292.41003294 262.37432163 ... 309.75708237 178.60858675\n",
      "    91.30498708]\n",
      "  [272.30698521 226.87485547 215.71591062 ... 243.81463014 180.85805279\n",
      "    76.1045633 ]]\n",
      "\n",
      " [[985.57680659 981.91867247 984.11016628 ... 952.22855739 949.23943372\n",
      "   947.49155024]\n",
      "  [983.28338927 984.96547147 986.46248799 ... 961.36846268 957.42533553\n",
      "   952.8068716 ]\n",
      "  [980.40621594 982.91463643 981.07757369 ... 964.78436734 960.33073039\n",
      "   953.57513307]\n",
      "  ...\n",
      "  [342.03233467 361.80366373 411.32532087 ... 294.95625374 206.38321195\n",
      "   148.2509364 ]\n",
      "  [446.29357715 361.61570026 307.80098125 ... 348.89649309 225.29408908\n",
      "   107.03752708]\n",
      "  [331.86379191 262.65226086 254.06064278 ... 272.38798485 215.58334858\n",
      "    99.84133836]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[784.65492649 780.88816417 781.67668769 ... 756.70392474 754.17235754\n",
      "   753.55823932]\n",
      "  [781.04139209 782.88882102 784.25621882 ... 764.24531746 760.50528321\n",
      "   757.42850668]\n",
      "  [779.74720414 781.90150787 780.37234392 ... 767.4903968  763.824111\n",
      "   758.9350415 ]\n",
      "  ...\n",
      "  [262.28780634 274.41330574 330.37513628 ... 227.32383214 166.59009751\n",
      "   121.18669062]\n",
      "  [357.90960368 296.47012584 250.42073929 ... 292.18718404 192.63730295\n",
      "    80.34832331]\n",
      "  [280.26694313 217.4450122  208.38686162 ... 223.57328461 186.70536228\n",
      "    69.84652569]]\n",
      "\n",
      " [[829.68588165 826.39725882 827.80884564 ... 801.25129308 798.80000181\n",
      "   797.86729513]\n",
      "  [826.63762375 829.6753696  831.26708709 ... 808.63494149 805.50233578\n",
      "   802.01477505]\n",
      "  [825.48790297 828.58142656 826.51108858 ... 812.52756076 808.76534151\n",
      "   803.4496672 ]\n",
      "  ...\n",
      "  [263.84509516 302.5174713  356.70009375 ... 245.39064267 181.67487866\n",
      "   130.35985759]\n",
      "  [391.06025867 312.85667411 270.17518705 ... 310.59223346 199.25112864\n",
      "    86.79921953]\n",
      "  [301.9702688  213.17358411 227.98810928 ... 247.39281358 187.82578202\n",
      "    72.44715036]]\n",
      "\n",
      " [[782.29615527 780.66810636 782.40436601 ... 755.76191047 754.35998141\n",
      "   752.2312155 ]\n",
      "  [780.53630842 783.39431958 784.23305361 ... 763.17292053 760.39104614\n",
      "   756.12952943]\n",
      "  [779.32733684 781.59880638 779.3838248  ... 766.18830618 763.2096217\n",
      "   756.9287566 ]\n",
      "  ...\n",
      "  [267.30237642 281.99484648 339.28213694 ... 229.87294961 146.28656898\n",
      "   128.10497907]\n",
      "  [357.78381525 276.17622426 248.93209531 ... 293.53027073 155.36672703\n",
      "    93.66755499]\n",
      "  [255.52996241 209.9017456  198.88208446 ... 235.68622956 154.83721161\n",
      "    74.452525  ]]]\n",
      "\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Flatten result: \n",
      "[538.83777986 537.83819278 539.4507759  ... 235.68622956 154.83721161\n",
      "  74.452525  ]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[     0.         315458.38965849      0.              0.\n",
      " 132259.64442221 319540.54627139 155362.82355214      0.        ]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[1.]\n",
      "\n",
      "Forward propagation result: \n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path)\n",
    "image_number = 0\n",
    "image_matrix = [image_matrix[image_number]]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.forward_propagate(image_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58496305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 2]\n",
      "  [5 0]]]\n",
      "[[[0 2]\n",
      "  [5 0]]]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[[-10, 2], [5, -5]]])\n",
    "\n",
    "\n",
    "model = Model()\n",
    "detect = model.DetectorLayer()\n",
    "detect_f = detect.detect(input_data)\n",
    "print(detect_f)\n",
    "\n",
    "err = np.array([[[-5, 2], [5, 4]]])\n",
    "\n",
    "back_f = detect.backward(err)\n",
    "print(back_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2c760ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil pooling forward\n",
      "[[[ 6.  8.]\n",
      "  [14. 16.]]]\n",
      "hasil pooling backward\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 2. 0. 1.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 1. 0. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "# Max pooling testing\n",
    "input_data = np.array([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]])\n",
    "polingLayer = model.PoolingLayer(2, 2)\n",
    "pol_result_f = polingLayer.pool(input_data)\n",
    "print(\"hasil pooling forward\")\n",
    "print(pol_result_f)\n",
    "\n",
    "print(\"hasil pooling backward\")\n",
    "error_data = np.array([[[2, 1], [1, 2]]])\n",
    "\n",
    "print(polingLayer.backward(error_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54964277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobot awal : \n",
      "[[ 0.63946067 -0.59992825  1.80377072]]\n",
      "bias awal : \n",
      "[0.]\n",
      "\n",
      "\n",
      "hasil forward1 : \n",
      "[3.33840786e-10 1.35222561e-16 1.39767048e-04]\n",
      "hasil forward2 : \n",
      "[0.00025211]\n",
      "\n",
      "\n",
      "hasil backward\n",
      "hasil dE : \n",
      "[ 0.00639461 -0.00599928  0.01803771]\n",
      "bobot update : \n",
      "[[ 0.06394607 -0.05999282  0.18037693]]\n",
      "bias update : \n",
      "[-0.001]\n"
     ]
    }
   ],
   "source": [
    "# Dense layer testing\n",
    "denseLayer1 = Model().DenseLayer(unit_count=3, activation=\"sigmoid\")\n",
    "denseLayer = Model().DenseLayer(unit_count=1, activation=\"relu\")\n",
    "flatLayer = Model().FlattenLayer()\n",
    "flat_f = flatLayer.flatten(pol_result_f)\n",
    "dense_f1 = denseLayer1.dense(flat_f)\n",
    "dense_f = denseLayer.dense(dense_f1)\n",
    "\n",
    "print(\"bobot awal : \")\n",
    "print(denseLayer._weight)\n",
    "\n",
    "print(\"bias awal : \")\n",
    "print(denseLayer._bias)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"hasil forward1 : \")\n",
    "print(dense_f1)\n",
    "\n",
    "print(\"hasil forward2 : \")\n",
    "print(dense_f)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"hasil backward\")\n",
    "\n",
    "# error_data = np.array([0.1, -0.2, 0.3])\n",
    "error_data = np.array([0.01])\n",
    "# Backward propagation\n",
    "dE = denseLayer.backward(error_data)\n",
    "print(\"hasil dE : \")\n",
    "print(dE)\n",
    "\n",
    "# Update bobot lapisan\n",
    "lr = 0.1\n",
    "denseLayer.update_weight(lr)\n",
    "print(\"bobot update : \")\n",
    "print(denseLayer._weight)\n",
    "print(\"bias update : \")\n",
    "print(denseLayer._bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
