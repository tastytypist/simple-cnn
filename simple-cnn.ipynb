{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional Neural Network\n",
    "\n",
    "# Simple CNN\n",
    "**Simple CNN** is a convolutional neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.241130300Z",
     "start_time": "2023-10-05T11:04:52.944944900Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.253382100Z",
     "start_time": "2023-10-05T11:04:53.246131700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model. In\n",
    "    addition, this class is also used to save and load the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model_object: \"Model\", file_name: str = \"model.json\") -> None:\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json = jsonpickle.encode(model_object, indent=4)\n",
    "            file.write(json)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str = \"model.json\") -> \"Model\":\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"r\") as file:\n",
    "            json = file.read()\n",
    "            return jsonpickle.decode(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a9981df315bc56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:04:53.315203200Z",
     "start_time": "2023-10-05T11:04:53.302381400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._result = []\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def forward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing forward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the backward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing backward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._biases = None\n",
    "\n",
    "        def _pad_weights(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "            padding_size: int,\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            self._weight_dimension = len(weights)\n",
    "\n",
    "            self._weight_height = (weight_height := len(weights[0])) + 2 * padding_size\n",
    "            self._weight_width = (weight_width := len(weights[0][0])) + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        weights[i][j - padding_size][k - padding_size]\n",
    "                        if padding_size <= j < weight_height + padding_size\n",
    "                        or padding_size <= k < weight_width + padding_size\n",
    "                        else 0.0\n",
    "                        for k in range(self._weight_width)\n",
    "                    ]\n",
    "                    for j in range(self._weight_height)\n",
    "                ]\n",
    "                for i in range(self._weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += np.sum(feature)\n",
    "            return feature_maps\n",
    "\n",
    "        def forward_propagate(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.convolute(weights)\n",
    "            print(\"Convolution result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "\n",
    "        # @staticmethod\n",
    "        def detect(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            self.input = feature\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def forward_propagate(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.detect(feature)\n",
    "            print(\"Detector result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def backward(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the detector layer.\n",
    "            Using Relu derivative : dReLU/dx = 1 if x > 0, otherwise 0\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            dX = error * (self.input > 0)\n",
    "            return dX\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "\n",
    "        def average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self.average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self.max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.pool(input_matrix)\n",
    "            print(\"Pooling result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def backward(self, error):\n",
    "            F, W, H = self.input.shape\n",
    "            dX = np.zeros(self.input.shape)\n",
    "            for i in range(0, F):\n",
    "                for j in range(0, W, self._filter_size):\n",
    "                    for k in range(0, H, self._filter_size):\n",
    "                        input_slice = self.input[i, j : j + self._filter_size, k : k + self._filter_size]\n",
    "                        max_input_slice = np.argmax(input_slice)\n",
    "                        max_idx = np.unravel_index(max_input_slice, (self._filter_size, self._filter_size))\n",
    "                        if ((j + max_idx[0]) < W and (k + max_idx[1]) < H):\n",
    "                            dX[i, j + max_idx[0], k + max_idx[1]] = error[i, int(j // self._filter_size) , int(k // self._filter_size)]\n",
    "            return dX\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._weight = []\n",
    "            self._deltaW = np.zeros((unit_count))\n",
    "\n",
    "        def dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            self.input = input_matrix\n",
    "            if(len(self._weight) == 0):\n",
    "                self._weight = np.random.randn(self._unit_count,len(self.input))\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                self.output =  expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                self.output = np.maximum(result, 0)\n",
    "            return self.output\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.dense(input_matrix)\n",
    "            print(\"Dense result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        def backward(self, error) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the layer.\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            derivative_value = np.array([])\n",
    "            for i in self.output:\n",
    "                derivative_value = np.append(derivative_value, self.derivative_act_func(self._activation, i))\n",
    "            \n",
    "            self._deltaW += np.multiply(derivative_value, error)\n",
    "            dE = np.matmul(error, self._weight)\n",
    "            return dE\n",
    "        \n",
    "        def update_weight(self, learning_rate, momentum) -> None:\n",
    "            \"\"\" \n",
    "            Indicate and perform the update weight and bias on the model\n",
    "            \"\"\"\n",
    "            for i in range(self._unit_count):\n",
    "                self._weight[i] = self._weight[i] - ((momentum * self._weight[i]) + (learning_rate * self._deltaW[i] * self.input))\n",
    "\n",
    "            self._bias = self._bias - ((momentum * self._bias) + (learning_rate * self._deltaW))\n",
    "            self._deltaW = np.zeros((self._unit_count))\n",
    "\n",
    "        def derivative_act_func(self, activation, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from activation function and input\n",
    "            \"\"\"\n",
    "            if(activation == \"sigmoid\"):\n",
    "                return self.sigmoid_detivative(input)\n",
    "            else:\n",
    "                return self.relu_derivative(input)\n",
    "            \n",
    "        def sigmoid_detivative(self, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            sigmoid = 1/(1+np.exp(-input))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "        \n",
    "        def relu_derivative(self, input):\n",
    "            \"\"\" \n",
    "            Take derivative value from input\n",
    "            \"\"\"\n",
    "            if(input >= 0):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "\n",
    "        @staticmethod\n",
    "        def flatten(input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self.flatten(input_matrix)\n",
    "            print(\"Flatten result: \")\n",
    "            print(result)\n",
    "            print()\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.forward_propagate(tensor)\n",
    "        print(\"Forward propagation result: \")\n",
    "        print(tensor)\n",
    "        self._result = tensor\n",
    "\n",
    "    def backward_propagate(self) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, tensor: npt.NDArray[npt.NDArray], epochs: int = 3, learning_rate: float = 0.01) -> None:\n",
    "        \"\"\"\n",
    "        Fit and train the CNN model.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        :param epochs: An integer specifying the number of training epochs.\n",
    "        :param learning_rate: A float specifying the learning rate of the model.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80fb427e45bb865f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T11:05:10.033906700Z",
     "start_time": "2023-10-05T11:04:53.312209300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on convolution layer...\n",
      "\n",
      "Convolution result: \n",
      "[[[769.02017098 758.48488251 753.34217096 ... 724.48659062 722.14527207\n",
      "   724.57089252]\n",
      "  [758.03316848 752.02760271 751.51777002 ... 727.10918233 724.3045902\n",
      "   726.33218871]\n",
      "  [754.48155303 749.99736019 752.025008   ... 728.38220058 725.54082674\n",
      "   727.14026082]\n",
      "  ...\n",
      "  [337.71553847 255.94649683 168.10441371 ...  40.06159871 103.53433015\n",
      "   173.19641294]\n",
      "  [242.2869486  150.32214102 127.39690888 ...  74.63678005 102.68406009\n",
      "   231.8819269 ]\n",
      "  [186.52771626 101.21085963 112.35303975 ... 112.82176885 107.44304893\n",
      "   249.01030455]]\n",
      "\n",
      " [[704.65194989 695.2839642  692.98219536 ... 665.94876067 662.81328035\n",
      "   666.96063431]\n",
      "  [694.93021444 689.94815722 690.1432308  ... 668.16714308 664.12572791\n",
      "   668.2242406 ]\n",
      "  [691.52807126 688.14260351 691.49661457 ... 669.09984677 665.25338278\n",
      "   668.38686855]\n",
      "  ...\n",
      "  [273.17032424 196.62421186 166.87838404 ...  33.59917327  98.8195717\n",
      "   173.88769401]\n",
      "  [233.90898655 153.40689161 115.26783126 ...  57.36834065 120.26354919\n",
      "   203.95206374]\n",
      "  [157.77870079  70.02312932  52.2802494  ...  90.49076975 114.75751474\n",
      "   186.74026613]]\n",
      "\n",
      " [[580.54861583 573.16005572 570.43430605 ... 547.4063199  545.03614563\n",
      "   548.22324257]\n",
      "  [572.22699152 568.40284916 568.0050344  ... 549.47225386 547.08557355\n",
      "   549.90169025]\n",
      "  [568.88170081 565.91630072 567.68444806 ... 549.40596353 548.10549297\n",
      "   549.74073811]\n",
      "  ...\n",
      "  [228.84862223 178.13860808 127.37072532 ...  31.02754643  74.29694341\n",
      "   161.3975282 ]\n",
      "  [194.64881702 135.10023491 121.12140473 ...  57.79120361  74.60512613\n",
      "   199.02012092]\n",
      "  [148.68085677  62.72426731  71.40184979 ...  94.79217203  71.95700876\n",
      "   198.14773359]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[867.89081287 856.72241188 852.18666109 ... 818.96411351 815.09816041\n",
      "   818.28969802]\n",
      "  [857.85818349 850.40627247 849.63595243 ... 821.01828443 816.88509042\n",
      "   819.38430331]\n",
      "  [852.95740565 848.00330799 849.65327979 ... 823.47630389 819.23102231\n",
      "   821.50805781]\n",
      "  ...\n",
      "  [376.45970096 265.54884751 180.2616283  ...  47.78573981  95.56016686\n",
      "   211.15177388]\n",
      "  [252.63613713 170.23631085 186.84470856 ...  93.30934811 118.80981048\n",
      "   289.05201208]\n",
      "  [229.00725026 125.87303942 108.64415276 ... 125.03548383 108.32176913\n",
      "   247.46290816]]\n",
      "\n",
      " [[817.05643966 807.4746559  803.50397661 ... 774.76131855 771.95354526\n",
      "   775.64149439]\n",
      "  [808.23725204 802.49299831 803.0095518  ... 775.59978214 772.11517534\n",
      "   776.24794432]\n",
      "  [805.87629263 801.72614235 804.38044533 ... 778.59342581 775.21220357\n",
      "   777.85453042]\n",
      "  ...\n",
      "  [365.0441401  258.87609686 165.75315891 ...  44.85051494 106.71142841\n",
      "   205.52706795]\n",
      "  [236.45230582 132.41976678 137.71836194 ... 104.18357857 118.87823125\n",
      "   283.47085973]\n",
      "  [167.48574883  91.82284859 108.37532178 ... 109.41012902 122.31691561\n",
      "   251.83559641]]\n",
      "\n",
      " [[649.25061629 639.46360634 637.21991996 ... 612.66538216 608.16509871\n",
      "   611.08722799]\n",
      "  [640.51030786 633.80133095 635.20719772 ... 614.07843035 609.56982179\n",
      "   612.55368645]\n",
      "  [636.29642886 631.22589313 634.8255069  ... 615.47255456 610.76512487\n",
      "   613.65732489]\n",
      "  ...\n",
      "  [281.422166   202.43987062 156.39976378 ...  26.46177086  81.96728798\n",
      "   147.40018396]\n",
      "  [209.04781593 135.47991966  99.91172179 ...  39.54131019 108.98318336\n",
      "   157.13280041]\n",
      "  [158.8019317   84.6319311   49.12054358 ...  65.47555726 115.06777911\n",
      "   137.83789615]]]\n",
      "\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Detector result: \n",
      "[[[769.02017098 758.48488251 753.34217096 ... 724.48659062 722.14527207\n",
      "   724.57089252]\n",
      "  [758.03316848 752.02760271 751.51777002 ... 727.10918233 724.3045902\n",
      "   726.33218871]\n",
      "  [754.48155303 749.99736019 752.025008   ... 728.38220058 725.54082674\n",
      "   727.14026082]\n",
      "  ...\n",
      "  [337.71553847 255.94649683 168.10441371 ...  40.06159871 103.53433015\n",
      "   173.19641294]\n",
      "  [242.2869486  150.32214102 127.39690888 ...  74.63678005 102.68406009\n",
      "   231.8819269 ]\n",
      "  [186.52771626 101.21085963 112.35303975 ... 112.82176885 107.44304893\n",
      "   249.01030455]]\n",
      "\n",
      " [[704.65194989 695.2839642  692.98219536 ... 665.94876067 662.81328035\n",
      "   666.96063431]\n",
      "  [694.93021444 689.94815722 690.1432308  ... 668.16714308 664.12572791\n",
      "   668.2242406 ]\n",
      "  [691.52807126 688.14260351 691.49661457 ... 669.09984677 665.25338278\n",
      "   668.38686855]\n",
      "  ...\n",
      "  [273.17032424 196.62421186 166.87838404 ...  33.59917327  98.8195717\n",
      "   173.88769401]\n",
      "  [233.90898655 153.40689161 115.26783126 ...  57.36834065 120.26354919\n",
      "   203.95206374]\n",
      "  [157.77870079  70.02312932  52.2802494  ...  90.49076975 114.75751474\n",
      "   186.74026613]]\n",
      "\n",
      " [[580.54861583 573.16005572 570.43430605 ... 547.4063199  545.03614563\n",
      "   548.22324257]\n",
      "  [572.22699152 568.40284916 568.0050344  ... 549.47225386 547.08557355\n",
      "   549.90169025]\n",
      "  [568.88170081 565.91630072 567.68444806 ... 549.40596353 548.10549297\n",
      "   549.74073811]\n",
      "  ...\n",
      "  [228.84862223 178.13860808 127.37072532 ...  31.02754643  74.29694341\n",
      "   161.3975282 ]\n",
      "  [194.64881702 135.10023491 121.12140473 ...  57.79120361  74.60512613\n",
      "   199.02012092]\n",
      "  [148.68085677  62.72426731  71.40184979 ...  94.79217203  71.95700876\n",
      "   198.14773359]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[867.89081287 856.72241188 852.18666109 ... 818.96411351 815.09816041\n",
      "   818.28969802]\n",
      "  [857.85818349 850.40627247 849.63595243 ... 821.01828443 816.88509042\n",
      "   819.38430331]\n",
      "  [852.95740565 848.00330799 849.65327979 ... 823.47630389 819.23102231\n",
      "   821.50805781]\n",
      "  ...\n",
      "  [376.45970096 265.54884751 180.2616283  ...  47.78573981  95.56016686\n",
      "   211.15177388]\n",
      "  [252.63613713 170.23631085 186.84470856 ...  93.30934811 118.80981048\n",
      "   289.05201208]\n",
      "  [229.00725026 125.87303942 108.64415276 ... 125.03548383 108.32176913\n",
      "   247.46290816]]\n",
      "\n",
      " [[817.05643966 807.4746559  803.50397661 ... 774.76131855 771.95354526\n",
      "   775.64149439]\n",
      "  [808.23725204 802.49299831 803.0095518  ... 775.59978214 772.11517534\n",
      "   776.24794432]\n",
      "  [805.87629263 801.72614235 804.38044533 ... 778.59342581 775.21220357\n",
      "   777.85453042]\n",
      "  ...\n",
      "  [365.0441401  258.87609686 165.75315891 ...  44.85051494 106.71142841\n",
      "   205.52706795]\n",
      "  [236.45230582 132.41976678 137.71836194 ... 104.18357857 118.87823125\n",
      "   283.47085973]\n",
      "  [167.48574883  91.82284859 108.37532178 ... 109.41012902 122.31691561\n",
      "   251.83559641]]\n",
      "\n",
      " [[649.25061629 639.46360634 637.21991996 ... 612.66538216 608.16509871\n",
      "   611.08722799]\n",
      "  [640.51030786 633.80133095 635.20719772 ... 614.07843035 609.56982179\n",
      "   612.55368645]\n",
      "  [636.29642886 631.22589313 634.8255069  ... 615.47255456 610.76512487\n",
      "   613.65732489]\n",
      "  ...\n",
      "  [281.422166   202.43987062 156.39976378 ...  26.46177086  81.96728798\n",
      "   147.40018396]\n",
      "  [209.04781593 135.47991966  99.91172179 ...  39.54131019 108.98318336\n",
      "   157.13280041]\n",
      "  [158.8019317   84.6319311   49.12054358 ...  65.47555726 115.06777911\n",
      "   137.83789615]]]\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "\n",
      "Pooling result: \n",
      "[[[755.43663188 752.92254806 754.81647621 ... 729.74424934 727.76139942\n",
      "   726.32617537]\n",
      "  [753.68634326 755.67646308 756.76057565 ... 736.72144255 734.03223659\n",
      "   730.23806697]\n",
      "  [752.18318263 754.14693324 752.3337307  ... 739.74091784 736.55196395\n",
      "   731.09178798]\n",
      "  ...\n",
      "  [257.64541704 281.30301691 322.81179029 ... 222.38134406 152.48518964\n",
      "   118.28855014]\n",
      "  [346.50839943 272.50177066 241.01726821 ... 271.4727569  162.10713531\n",
      "    86.27900654]\n",
      "  [251.83555686 201.86177963 196.1150933  ... 214.52637446 155.48356206\n",
      "    77.53438753]]\n",
      "\n",
      " [[693.23411125 692.02907391 693.94579802 ... 670.76082344 669.30136961\n",
      "   667.50491841]\n",
      "  [692.36208236 695.02591334 695.96193898 ... 677.09909449 675.05961956\n",
      "   671.03309657]\n",
      "  [691.28709518 693.51916684 691.47367836 ... 679.85377647 677.06266743\n",
      "   671.50755322]\n",
      "  ...\n",
      "  [233.13606999 260.19139508 302.5518959  ... 210.69251253 138.14204318\n",
      "   111.99503629]\n",
      "  [325.19501891 246.14033684 224.57097007 ... 261.28510284 143.65692399\n",
      "    77.89837186]\n",
      "  [229.80652102 178.39399647 182.79841405 ... 208.64452586 138.26820537\n",
      "    66.40158733]]\n",
      "\n",
      " [[570.58447803 568.87664787 570.36253277 ... 551.11706968 550.20363336\n",
      "   548.90587514]\n",
      "  [569.3004511  570.97357392 572.16277807 ... 556.42600165 554.41472425\n",
      "   551.5393162 ]\n",
      "  [568.58923272 569.99340998 568.9979727  ... 559.09338816 556.66176269\n",
      "   552.26421172]\n",
      "  ...\n",
      "  [185.51298617 209.15448107 251.0442493  ... 164.81969001 111.90974677\n",
      "    91.40330494]\n",
      "  [264.89375807 209.09353786 183.64865012 ... 206.47515533 117.88314135\n",
      "    69.74093259]\n",
      "  [196.51358642 148.13101963 147.26874195 ... 169.27702454 111.15863499\n",
      "    56.95242684]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[853.92380974 851.00222314 852.66652448 ... 824.61905071 822.46805652\n",
      "   820.5327255 ]\n",
      "  [851.51043773 853.27546799 854.51711391 ... 832.67401569 829.31382295\n",
      "   825.14485669]\n",
      "  [849.54537819 851.69167186 849.83553666 ... 835.47869531 831.92445577\n",
      "   825.89462125]\n",
      "  ...\n",
      "  [299.66620943 305.88932098 359.25833886 ... 252.99966715 170.02239053\n",
      "   132.86863118]\n",
      "  [383.51726106 306.05267747 269.43320213 ... 306.25881517 184.23616501\n",
      "    96.16109631]\n",
      "  [283.08570801 232.37992536 218.05277775 ... 243.60044573 182.08528346\n",
      "    85.84912668]]\n",
      "\n",
      " [[805.97308385 804.78166437 807.23614218 ... 780.34646896 778.32444863\n",
      "   776.17070495]\n",
      "  [805.37180008 807.73485158 808.34944947 ... 787.65972696 784.91933864\n",
      "   780.59351778]\n",
      "  [802.78194766 805.34782998 803.25718707 ... 789.98307835 786.46823489\n",
      "   780.54823919]\n",
      "  ...\n",
      "  [287.68482622 309.96421412 337.73264943 ... 247.76533607 163.68799868\n",
      "   125.3829525 ]\n",
      "  [368.9069946  279.9138102  252.59754939 ... 285.45201075 165.92389275\n",
      "    93.08568471]\n",
      "  [253.02347876 208.40958732 208.51857009 ... 228.05133846 161.42080977\n",
      "    85.63392709]]\n",
      "\n",
      " [[637.53342311 635.93153364 637.2294331  ... 616.24109559 614.8581752\n",
      "   613.42341931]\n",
      "  [635.68841189 638.47111119 639.38407687 ... 621.92545051 619.82910974\n",
      "   616.49956791]\n",
      "  [634.85040093 637.40645071 635.38444803 ... 624.71350866 622.223938\n",
      "   617.42987195]\n",
      "  ...\n",
      "  [213.65124243 233.91996401 274.16524766 ... 192.40811415 131.90350102\n",
      "   103.77859646]\n",
      "  [297.96254375 227.86259032 208.98962275 ... 244.41241904 140.54284846\n",
      "    67.45149265]\n",
      "  [217.30059895 168.44128761 173.23551927 ... 193.48833435 138.48889319\n",
      "    57.6093713 ]]]\n",
      "\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Flatten result: \n",
      "[755.43663188 752.92254806 754.81647621 ... 193.48833435 138.48889319\n",
      "  57.6093713 ]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[ 99082.6114442   67013.99173149 760722.42034842 634445.81782313\n",
      "      0.              0.              0.         263926.04068246]\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Dense result: \n",
      "[0.]\n",
      "\n",
      "Forward propagation result: \n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path)\n",
    "image_number = 0\n",
    "image_matrix = [image_matrix[image_number]]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.forward_propagate(image_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58496305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 2]\n",
      "  [5 0]]]\n",
      "[[[0 2]\n",
      "  [5 0]]]\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([[[-10, 2],\n",
    "                       [5, -5]]])\n",
    "\n",
    "\n",
    "model = Model()\n",
    "detect = model.DetectorLayer()\n",
    "detect_f = detect.detect(input_data)\n",
    "print(detect_f)\n",
    "\n",
    "error = np.array([[[-5, 2],\n",
    "                   [5, 4]]])\n",
    "\n",
    "back_f = detect.backward(error)\n",
    "print(back_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2c760ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil pooling forward\n",
      "[[[ 6.  8.]\n",
      "  [14. 16.]]]\n",
      "hasil pooling backward\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 2. 0. 1.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 1. 0. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "#max pooling testing\n",
    "input_data = np.array([[[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12],\n",
    "                       [13, 14, 15, 16]]])\n",
    "polingLayer = model.PoolingLayer(2,2)\n",
    "pol_result_f = polingLayer.pool(input_data)\n",
    "print(\"hasil pooling forward\")\n",
    "print(pol_result_f)\n",
    "\n",
    "print(\"hasil pooling backward\")\n",
    "error_data = np.array([[[2, 1],\n",
    "                        [1, 2]]])\n",
    "\n",
    "print(polingLayer.backward(error_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54964277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bobot awal : \n",
      "[[ 0.59881562 -0.70563249  1.90341179]]\n",
      "bias awal : \n",
      "[0.]\n",
      "\n",
      "\n",
      "hasil forward1 : \n",
      "[0.00079703 0.00033129 0.00174487]\n",
      "hasil forward2 : \n",
      "[0.00356472]\n",
      "\n",
      "\n",
      "hasil backward\n",
      "hasil dE : \n",
      "[ 0.00598816 -0.00705632  0.01903412]\n",
      "bobot update : \n",
      "[[ 0.05988076 -0.07056358  0.19033943]]\n",
      "bias update : \n",
      "[-0.001]\n"
     ]
    }
   ],
   "source": [
    "#Dense layer testing\n",
    "denseLayer1 = Model().DenseLayer(unit_count=3, activation=\"sigmoid\")\n",
    "denseLayer = Model().DenseLayer(unit_count=1, activation=\"relu\")\n",
    "flatLayer = Model().FlattenLayer()\n",
    "flat_f = flatLayer.flatten(pol_result_f)\n",
    "dense_f1 = denseLayer1.dense(flat_f)\n",
    "dense_f = denseLayer.dense(dense_f1)\n",
    "\n",
    "print(\"bobot awal : \")\n",
    "print(denseLayer._weight)\n",
    "\n",
    "print(\"bias awal : \")\n",
    "print(denseLayer._bias)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"hasil forward1 : \")\n",
    "print(dense_f1)\n",
    "\n",
    "print(\"hasil forward2 : \")\n",
    "print(dense_f)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"hasil backward\")\n",
    "\n",
    "# error_data = np.array([0.1, -0.2, 0.3])\n",
    "error_data = np.array([0.01])\n",
    "# Backward propagation\n",
    "dE = denseLayer.backward(error_data)\n",
    "print(\"hasil dE : \")\n",
    "print(dE)\n",
    "\n",
    "#Update bobot lapisan\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "denseLayer.update_weight(learning_rate, momentum)\n",
    "print(\"bobot update : \")\n",
    "print(denseLayer._weight)\n",
    "print(\"bias update : \")\n",
    "print(denseLayer._bias)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
