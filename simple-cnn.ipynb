{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-18T05:25:04.730297900Z",
     "start_time": "2023-09-18T05:25:04.598416100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T05:25:04.741437600Z",
     "start_time": "2023-09-18T05:25:04.731294500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        folder_path = []\n",
    "        class_label = np.array([], dtype=np.int16)\n",
    "        class_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [\n",
    "                os.path.join(class_folder_path, image_name)\n",
    "                for image_name in list_image_name\n",
    "            ]\n",
    "\n",
    "            folder_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            class_label = np.concatenate((class_label, temp_class_label), axis=0)\n",
    "            class_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(folder_path), class_label, class_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(folder_path: str) -> npt.NDArray:\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in folder_path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            image_matrix = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image_matrix = cv2.resize(image_matrix, size)\n",
    "            list_of_image_matrix.append(image_matrix)\n",
    "\n",
    "        # The returned NDArray is transposed from (batch_size, height, width,\n",
    "        # channels) into (batch_size, channels, height, width) to ease further\n",
    "        # operations\n",
    "        return np.array(list_of_image_matrix).transpose(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "# For testing purposes\n",
    "# if __name__ == \"__main__\":\n",
    "#     folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "#     print(Utils.convert_image_to_matrix(folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9981df315bc56c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T05:25:04.764162300Z",
     "start_time": "2023-09-18T05:25:04.740439700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: list[list[int]] | None = None,\n",
    "        padding_size: int | None = None,\n",
    "        weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]] | None = None,\n",
    "        conv_filter_count: int | None = None,\n",
    "        conv_filter_size: list[tuple[int]] | None = None,\n",
    "        conv_stride_size: list[int] | None = None,\n",
    "        pool_filter_size: tuple[int] | None = None,\n",
    "        pool_stride_size: int | None = None,\n",
    "        pool_mode: str | None = None,\n",
    "        dense_unit_count: int | None = None,\n",
    "        dense_activation: str | None = None,\n",
    "    ) -> None:\n",
    "        self._input_size = input_size\n",
    "        self._padding_size = padding_size\n",
    "        self._weights = weights\n",
    "        self._conv_filter_count = conv_filter_count\n",
    "        self._conv_filter_size = conv_filter_size\n",
    "        self._conv_stride_size = conv_stride_size\n",
    "        self._pool_filter_size = pool_filter_size\n",
    "        self._pool_stride_size = pool_stride_size\n",
    "        self._pool_mode = pool_mode\n",
    "        self._dense_unit_count = dense_unit_count\n",
    "        self._dense_activation = dense_activation\n",
    "        # self._convolution_layers = [\n",
    "        #     self.ConvolutionLayer(self._conv_filter_size, self._conv_stride_size)\n",
    "        #     for _ in range(self._conv_filter_count)\n",
    "        # ]\n",
    "        # self._detector_layer = self.DetectorLayer()\n",
    "        # self._pooling_layer = self.PoolingLayer(\n",
    "        #     self._pool_filter_size, self._pool_stride_size\n",
    "        # )\n",
    "        # self._dense_layer = self.DenseLayer(\n",
    "        #     self._dense_unit_count, self._dense_activation\n",
    "        # )\n",
    "        # self._flatten_layer = self.FlattenLayer()\n",
    "\n",
    "    class ConvolutionLayer:\n",
    "        def __init__(\n",
    "            self, filter_size: list[tuple[int]], stride_size: list[int]\n",
    "        ) -> None:\n",
    "            self._filter_weight = np.array(\n",
    "                [\n",
    "                    [\n",
    "                        [random.random() for _ in range(filter_size[i][1])]\n",
    "                        for _ in range(filter_size[i][0])\n",
    "                    ]\n",
    "                    for i in range(len(filter_size))\n",
    "                ]\n",
    "            )\n",
    "            self._stride_size = stride_size\n",
    "\n",
    "        def convolute(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            feature_maps = []\n",
    "            print(self._filter_weight)\n",
    "            for i in range(len(weights)):\n",
    "                feature_map = []\n",
    "                for j in range(\n",
    "                    0,\n",
    "                    len(weights[i]) - len(self._filter_weight[i]) + 1,\n",
    "                    self._stride_size[i],\n",
    "                ):\n",
    "                    feature_row = []\n",
    "                    for k in range(\n",
    "                        0,\n",
    "                        len(weights[i][j]) - len(self._filter_weight[i][0]) + 1,\n",
    "                        self._stride_size[i],\n",
    "                    ):\n",
    "                        field = weights[\n",
    "                            i,\n",
    "                            j : j + len(self._filter_weight[i]),\n",
    "                            k : k + len(self._filter_weight[i][0]),\n",
    "                        ]\n",
    "                        feature = field * self._filter_weight[i]\n",
    "                        feature_row.append(np.sum(feature))\n",
    "                    feature_map.append(feature_row)\n",
    "                feature_maps.append(feature_map)\n",
    "            return np.array(feature_maps)\n",
    "\n",
    "    class DetectorLayer:\n",
    "        @classmethod\n",
    "        def detect(cls, feature: npt.NDArray) -> npt.NDArray:\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "    class PoolingLayer:\n",
    "        def __init__(\n",
    "            self, filter_size: tuple[int], stride_size: int, mode: str = \"max\"\n",
    "        ) -> None:\n",
    "            self.filter_size = filter_size\n",
    "            self.stride_size = stride_size\n",
    "            self.mode = mode\n",
    "\n",
    "        def average(\n",
    "            self, input_matrix: npt.NDArray, d: int, h: int, w: int\n",
    "        ) -> npt.NDArray:\n",
    "            h_start = h * self.stride_size\n",
    "            w_start = w * self.stride_size\n",
    "            h_end = h_start + self.filter_size\n",
    "            w_end = w_start + self.filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> npt.NDArray:\n",
    "            h_start = h * self.stride_size\n",
    "            w_start = w * self.stride_size\n",
    "            h_end = h_start + self.filter_size\n",
    "            w_end = w_start + self.filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self.filter_size) // self.stride_size + 1\n",
    "            filter_width = (width - self.filter_size) // self.stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self.mode == \"average\":\n",
    "                            pooled[d, h, w] = self.average(input_matrix, d, h, w)\n",
    "                        elif self.mode == \"max\":\n",
    "                            pooled[d, h, w] = self.max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "    class DenseLayer:\n",
    "        def __init__(self, units: int, activation: str = \"sigmoid\") -> None:\n",
    "            self.units = units\n",
    "            self.activation = activation\n",
    "            self.bias = np.zeros(units)\n",
    "            self.weight = np.random.randn(units)\n",
    "\n",
    "        def dense(self, input_matrix: npt.NDArray) -> None:\n",
    "            result = np.zeros(self.units)\n",
    "\n",
    "            for i in range(self.units):\n",
    "                input_weight = np.sum(self.weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self.bias[i]\n",
    "\n",
    "            if self.activation == \"sigmoid\":\n",
    "                return 1 / (1 + np.exp(-result))\n",
    "            elif self.activation == \"relu\":\n",
    "                return np.maximum(result, 0)\n",
    "\n",
    "    class FlattenLayer:\n",
    "        @classmethod\n",
    "        def flatten(cls, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "    def _pad_weights(self) -> None:\n",
    "        for i in range(len(self._weights)):\n",
    "            new_weight = [\n",
    "                [\n",
    "                    0.0 if j == 0 or k == 0 else self._weights[j - 1][k - 1]\n",
    "                    for k in range(len(self._weights[i][j]))\n",
    "                ]\n",
    "                for j in range(len(self._weights[i]))\n",
    "            ]\n",
    "            self._weights = new_weight\n",
    "\n",
    "    def feedforward(self) -> None:\n",
    "        self._pad_weights()\n",
    "        self._convolution_layers.convolute()\n",
    "        self._detector_layer.detect()\n",
    "        self._pooling_layer.pool()\n",
    "        self._dense_layer.dense()\n",
    "        self._flatten_layer.flatten()\n",
    "\n",
    "    def back_propagate(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fb427e45bb865f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T05:25:04.774385200Z",
     "start_time": "2023-09-18T05:25:04.762155500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5459079  0.50246247 0.72569387]\n",
      "  [0.67244027 0.12517759 0.07082068]\n",
      "  [0.44350123 0.31518855 0.30526386]]]\n",
      "Convolution layer :\n",
      "[[[ 24.83091689  28.53737332  32.24382976  35.9502862 ]\n",
      "  [ 47.06965551  50.77611194  54.48256838  58.18902482]\n",
      "  [ 69.30839413  73.01485056  76.721307    80.42776344]\n",
      "  [ 91.54713275  95.25358918  98.96004562 102.66650206]]]\n",
      "Pooling layer :\n",
      "[[[ 50.77611194  58.18902482]\n",
      "  [ 95.25358918 102.66650206]]]\n",
      "Dense layer :\n",
      "[116.61404358  75.0330223    0.           0.        ]\n",
      "Flatten layer :\n",
      "[116.61404358  75.0330223    0.           0.        ]\n"
     ]
    }
   ],
   "source": [
    "input_tensor = np.array(\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3, 4, 5, 6], \n",
    "            [7, 8, 9, 10, 11, 12], \n",
    "            [13, 14, 15, 16, 17, 18], \n",
    "            [19, 20, 21, 22, 23, 24], \n",
    "            [25, 26, 27, 28, 29, 30], \n",
    "            [31, 32, 33, 34, 35, 36]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "model = Model()\n",
    "convolution_layer = model.ConvolutionLayer([(3, 3)], [1])\n",
    "convolution_result = convolution_layer.convolute(input_tensor)\n",
    "print(\"Convolution layer :\")\n",
    "print(convolution_result)\n",
    "pooling_layer = model.PoolingLayer(2, 2, \"max\")\n",
    "pool_result = pooling_layer.pool(convolution_result)\n",
    "print(\"Pooling layer :\")\n",
    "print(pool_result)\n",
    "dense = model.DenseLayer(4, \"relu\")\n",
    "dense_result = dense.dense(pool_result)\n",
    "print(\"Dense layer :\")\n",
    "print(dense_result)\n",
    "flatten = model.FlattenLayer()\n",
    "print(\"Flatten layer :\")\n",
    "print(flatten.flatten(dense_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
