{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional dan Long Short-Term Memory Neural Network\n",
    "\n",
    "# Simple CNN-LSTM\n",
    "**Simple CNN-LSTM** is a convolutional and long short-term neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1 - CNN)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n",
    "\n",
    "## Contribution (Milestone 2 - CNN)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Model training; Detector, Pooling, Dense, and Flatten layer; Report   |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Model training; Model loading and saving; Convolutional layer; Report |\n",
    "\n",
    "## Contribution (Milestone 1 - LSTM)\n",
    "| NIM      | Name                   | Contribution(s) |\n",
    "|----------|------------------------|-----------------|\n",
    "| 13520041 | Ilham Pratama          |                 |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon |                 |\n",
    "\n",
    "## Contribution (Milestone 2 - LSTM)\n",
    "| NIM      | Name                   | Contribution(s) |\n",
    "|----------|------------------------|-----------------|\n",
    "| 13520041 | Ilham Pratama          |                 |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon |                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import\n",
    "\n",
    "The following external library is used for the building of this model.\n",
    "1. `cv2` for preprocessing the image dataset\n",
    "2. `jsonpickle` for saving and loading the model\n",
    "3. `numpy` for performing model-related calculations\n",
    "4. `scipy` for performing a suppressed version of the logistic sigmoid function\n",
    "5. `sklearn` for computing the model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:11:35.662007700Z",
     "start_time": "2023-11-03T09:11:33.459678500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import expit\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:11:48.926993800Z",
     "start_time": "2023-11-03T09:11:48.866776300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model. In\n",
    "    addition, this class is also used to save and load the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model_object: \"Model\", file_name: str = \"model.json\") -> None:\n",
    "        \"\"\"\n",
    "        Save the specified model into a JSON file.\n",
    "\n",
    "        :param model_object: The model to be saved.\n",
    "        :param file_name: A string specifying the file name of the saved model.\n",
    "        \"\"\"\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json = jsonpickle.encode(model_object, indent=4)\n",
    "            file.write(json)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str = \"model.json\") -> \"Model\":\n",
    "        \"\"\"\n",
    "        Load a model from the specified JSON file name.\n",
    "\n",
    "        :param file_name: A string specifying the file name of the model to be\n",
    "                          loaded.\n",
    "        :return: The loaded model from the specified file.\n",
    "        \"\"\"\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"r\") as file:\n",
    "            json = file.read()\n",
    "            return jsonpickle.decode(json)\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_sequences(data, seq_length):\n",
    "        sequences = []\n",
    "        targets = []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            sequences.append(data[i:i+seq_length])\n",
    "            targets.append(data[i+seq_length])\n",
    "        return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation\n",
    "\n",
    "The convolutional model is represented by a class named `Model`. The `Model` class contains several inner class that represents all possible layers that the model can have. Such layers include the convolution layer (represented by the `ConvolutionLayer` class), the detector layer (represented by the `DetectorLayer` class), the pooling layer (represented by the `PoolingLayer` class), the dense layer (represented by the `DenseLayer` class), and the flatten layer (represented by the `FlattenLayer` class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9981df315bc56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:11:52.840643400Z",
     "start_time": "2023-11-03T09:11:52.639460500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._forward_result = None\n",
    "        self._backward_result = None\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def print_info(self):\n",
    "            print(f\"Layer {self._name}\")\n",
    "\n",
    "        def forward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing forward propagation on {self._name} layer...\\n\")\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the backward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing backward propagation on {self._name} layer...\\n\")\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._weights = None\n",
    "            self._biases = None\n",
    "            self._filter_gradients = []\n",
    "            self._bias_gradients = []\n",
    "\n",
    "        def _pad_weights(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]], padding_size: int, forward: bool = True\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param forward: A boolean specifying whether the padding is\n",
    "                            performed during forward propagation.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            weight_dimension = len(weights)\n",
    "            weight_height = len(weights[0])\n",
    "            weight_width = len(weights[0][0])\n",
    "\n",
    "            if forward:\n",
    "                self._weight_dimension = weight_dimension\n",
    "                self._weight_height = weight_height + 2 * padding_size\n",
    "                self._weight_width = weight_width + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        0.0\n",
    "                        if k < padding_size\n",
    "                        or k >= weight_width + padding_size\n",
    "                        or j < padding_size\n",
    "                        or j >= weight_height + padding_size\n",
    "                        else weights[i][j - padding_size][k - padding_size]\n",
    "                        for k in range(weight_width + 2 * padding_size)\n",
    "                    ]\n",
    "                    for j in range(weight_height + 2 * padding_size)\n",
    "                ]\n",
    "                for i in range(weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def _convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._weights = np.array(weights)\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += np.sum(feature)\n",
    "            return feature_maps\n",
    "\n",
    "        def _calculate_gradient(self, output_gradient: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Calculate the gradient used for updating the weight of the\n",
    "            convolution layer.\n",
    "\n",
    "            :param output_gradient: The gradient of the model's output with\n",
    "                                    respect to the layer ahead.\n",
    "            :return: The gradient of the model's output with respect to this\n",
    "                     convolutional layer.\n",
    "            \"\"\"\n",
    "            output_gradient_height = len(output_gradient[0])\n",
    "            output_gradient_width = len(output_gradient[0][0])\n",
    "\n",
    "            filter_gradient = np.zeros(\n",
    "                (self._filter_count, self._filter_dimension, self._filter_height, self._filter_width)\n",
    "            )\n",
    "            input_gradient = np.zeros((self._weight_dimension, self._weight_height, self._weight_width))\n",
    "            padded_output_gradient = self._pad_weights(output_gradient, 2, forward=False)\n",
    "\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(self._filter_dimension):\n",
    "                    for k in range(0, self._weight_height - output_gradient_height + 1, self._stride_height):\n",
    "                        for l in range(0, self._weight_width - output_gradient_width + 1, self._stride_width):\n",
    "                            field = self._weights[j, k : k + output_gradient_height, l : l + output_gradient_width]\n",
    "                            gradient = field * output_gradient[i]\n",
    "                            filter_gradient[i][j][k][l] = np.sum(gradient)\n",
    "                    for k in range(0, output_gradient_height - self._filter_height + 1, self._stride_height):\n",
    "                        for l in range(0, output_gradient_width - self._filter_width + 1, self._stride_width):\n",
    "                            field = padded_output_gradient[i, k : k + self._filter_height, l : l + self._filter_width]\n",
    "                            gradient = field * np.rot90(self._filter_weights[i][j], k=2)\n",
    "                            input_gradient[j][k][l] += np.sum(gradient)\n",
    "\n",
    "            self._filter_gradients.append(filter_gradient)\n",
    "            self._bias_gradients.append(output_gradient)\n",
    "\n",
    "            return input_gradient\n",
    "\n",
    "        # def print_info(self):\n",
    "        #     super().print_info()\n",
    "        #     print(f\"Output shape: \")\n",
    "        #     print(f\"Parameter count: \\n\")\n",
    "        #     return 1\n",
    "\n",
    "        def forward_propagate(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._convolute(weights)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            \"\"\"\n",
    "            Update the filter weights of the convolution layer.\n",
    "\n",
    "            :param learning_rate: A float specifying the learning rate of the model.\n",
    "            \"\"\"\n",
    "            self._filter_weights -= learning_rate * np.average(np.array(self._filter_gradients), axis=0)\n",
    "            self._biases -= learning_rate * np.average(np.array(self._bias_gradients), axis=0)\n",
    "            self._filter_gradients = []\n",
    "            self._bias_gradients = []\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "            self._weights = None\n",
    "\n",
    "        def _detect(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            self._weights = feature\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def _calculate_gradient(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the detector layer.\n",
    "            Use reLu derivative: dreLU/dx = 1 if x > 0, otherwise 0.\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            dx = error * (self._weights > 0)\n",
    "            return dx\n",
    "\n",
    "        # def print_info(self):\n",
    "        #     super().print_info()\n",
    "        #     print(f\"Output shape: {input_shape}\")\n",
    "        #     print(f\"Parameter count: 0\\n\")\n",
    "        #     return 1\n",
    "\n",
    "        def forward_propagate(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._detect(feature)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            \"\"\"\n",
    "            Update the filter weights of the detector layer.\n",
    "\n",
    "            Because the detector layer has no trainable weights, this method\n",
    "            exist for the purpose of iteratively updating the weights of all\n",
    "            the layers in the model. In practice, this method does nothing.\n",
    "\n",
    "            :param learning_rate: A float specifying the learning rate of the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "            self._weights = None\n",
    "\n",
    "        def _average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def _max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def _pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            self._weights = input_matrix\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self._average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self._max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def _calculate_gradient(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            f, w, h = self._weights.shape\n",
    "            dx = np.zeros(self._weights.shape)\n",
    "            for i in range(0, f):\n",
    "                for j in range(0, w, self._filter_size):\n",
    "                    for k in range(0, h, self._filter_size):\n",
    "                        input_slice = self._weights[i, j : j + self._filter_size, k : k + self._filter_size]\n",
    "                        max_input_slice = np.argmax(input_slice)\n",
    "                        max_idx = np.unravel_index(max_input_slice, (self._filter_size, self._filter_size))\n",
    "                        if (j + max_idx[0]) < w and (k + max_idx[1]) < h:\n",
    "                            dx[i, j + max_idx[0], k + max_idx[1]] = error[\n",
    "                                i, int(j // self._filter_size), int(k // self._filter_size)\n",
    "                            ]\n",
    "            return dx\n",
    "\n",
    "        # def print_info(self):\n",
    "        #     super().print_info()\n",
    "        #     print(f\"Output shape: \")\n",
    "        #     print(f\"Parameter count: 0\\n\")\n",
    "        #     return 1\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._pool(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            \"\"\"\n",
    "            Update the filter weights of the pooling layer.\n",
    "\n",
    "            Because the pooling layer has no trainable weights, this method\n",
    "            exist for the purpose of iteratively updating the weights of all\n",
    "            the layers in the model. In practice, this method does nothing.\n",
    "\n",
    "            :param learning_rate: A float specifying the learning rate of the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class LSTMLayer(Layer):\n",
    "        def __init__(self, cell_count: int, input_shape: (int, int)) -> None:\n",
    "            super().__init__(\"LSTM\")\n",
    "            self._cell_count = cell_count\n",
    "            self._input_shape = input_shape\n",
    "            # Parameters h and c, in the beginning the value : 0.0\n",
    "            self._h = np.zeros((cell_count, 1))\n",
    "            self._c = np.zeros((cell_count, 1))\n",
    "            # Parameters for the cell state\n",
    "            self._uc = np.random.rand(cell_count, input_shape[1])\n",
    "            self._wc = np.random.rand(cell_count, cell_count)\n",
    "            self._bc = np.random.rand(cell_count, 1)\n",
    "            # Parameters for the forget gate\n",
    "            self._uf = np.random.rand(cell_count, input_shape[1])\n",
    "            self._wf = np.random.rand(cell_count, cell_count)\n",
    "            self._bf = np.random.rand(cell_count, 1)\n",
    "            # Parameters for the input gate\n",
    "            self._ui = np.random.rand(cell_count, input_shape[1])\n",
    "            self._wi = np.random.rand(cell_count, cell_count)\n",
    "            self._bi = np.random.rand(cell_count, 1)\n",
    "            # Parameters for the output gate\n",
    "            self._uo = np.random.rand(cell_count, input_shape[1])\n",
    "            self._wo = np.random.rand(cell_count, cell_count)\n",
    "            self._bo = np.random.rand(cell_count, 1)\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid(x) -> float:\n",
    "            return 1 / (1 + math.exp(-x))\n",
    "\n",
    "        @staticmethod\n",
    "        def _tanh(x) -> float:\n",
    "            return math.tanh(x)\n",
    "\n",
    "        def _forget(self, input_matrix: npt.NDArray[npt.NDArray[float]]) -> npt.NDArray:\n",
    "            sigmoid = np.vectorize(self._sigmoid)\n",
    "            return sigmoid(self._uf @ input_matrix + self._wf @ self._h + self._bf)\n",
    "\n",
    "        def _input(self, input_matrix: npt.NDArray[npt.NDArray[float]]) -> tuple[npt.NDArray, npt.NDArray]:\n",
    "            sigmoid = np.vectorize(self._sigmoid)\n",
    "            tanh = np.vectorize(self._tanh)\n",
    "            it = sigmoid(self._ui @ input_matrix + self._wi @ self._h + self._bi)\n",
    "            ct = tanh(self._uc @ input_matrix + self._wc @ self._h + self._bc)\n",
    "            return it, ct\n",
    "\n",
    "        def _output(self, input_matrix: npt.NDArray[npt.NDArray[float]]) -> npt.NDArray:\n",
    "            sigmoid = np.vectorize(self._sigmoid)\n",
    "            return sigmoid(self._uo @ input_matrix + self._wo @ self._h + self._bo)\n",
    "\n",
    "        def _memorise(self, input_matrix: npt.NDArray[npt.NDArray[npt.NDArray[float]]]) -> npt.NDArray:\n",
    "            tanh = np.vectorize(self._tanh)\n",
    "\n",
    "            for sequence in input_matrix:\n",
    "                sequence_matrix = np.reshape(sequence, (self._input_shape[1], self._input_shape[0]))\n",
    "                forget_gate_matrix = self._forget(sequence_matrix)\n",
    "                input_gate_matrix, cell_candidate_matrix = self._input(sequence_matrix)\n",
    "                output_gate_matrix = self._output(sequence_matrix)\n",
    "\n",
    "                self._c = forget_gate_matrix * self._c + input_gate_matrix * cell_candidate_matrix\n",
    "                self._h = output_gate_matrix * tanh(self._c)\n",
    "\n",
    "            return self._h\n",
    "\n",
    "        def print_info(self, input_shape: None):\n",
    "            super().print_info()\n",
    "            print(f\"Layer name : lstm\")\n",
    "            print(f\"Output shape: (None, {self._cell_count})\")\n",
    "            parameter_count = 4 * ((self._input_shape[1] + self._cell_count) * self._cell_count + self._cell_count)\n",
    "            print(f\"Parameter count: {parameter_count}\")\n",
    "            print(\"#################################\")\n",
    "            return self._cell_count, parameter_count\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            super().forward_propagate()\n",
    "            result = self._memorise(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"\n",
    "            To be implemented later.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def update_weight(self) -> None:\n",
    "            \"\"\"\n",
    "            To be implemented later.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._dense_weight = []\n",
    "            self._weights = None\n",
    "            self._output = 0.0\n",
    "            self._deltaW = np.zeros(unit_count)\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid_derivative(input_: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Take derivative value from input.\n",
    "            \"\"\"\n",
    "            sigmoid = 1 / (1 + np.exp(-input_))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "\n",
    "        @staticmethod\n",
    "        def _relu_derivative(input_: npt.NDArray) -> int:\n",
    "            \"\"\"\n",
    "            Take derivative value from input.\n",
    "            \"\"\"\n",
    "            if input_ >= 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        def _derivative_act_func(self, activation: str, input_: npt.NDArray) -> npt.NDArray | float:\n",
    "            \"\"\"\n",
    "            Take derivative value from activation function and input.\n",
    "            \"\"\"\n",
    "            if activation == \"sigmoid\":\n",
    "                return self._sigmoid_derivative(input_)\n",
    "            else:\n",
    "                return self._relu_derivative(input_)\n",
    "\n",
    "        def _dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            self._weights = input_matrix\n",
    "            if len(self._dense_weight) == 0:\n",
    "                self._dense_weight = np.random.randn(self._unit_count, len(self._weights))\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._dense_weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                self.output = expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                self.output = np.maximum(result, 0)\n",
    "            return self.output\n",
    "\n",
    "        def _calculate_gradient(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the layer.\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            derivative_value = np.array([])\n",
    "            for i in self.output:\n",
    "                derivative_value = np.append(derivative_value, self._derivative_act_func(self._activation, i))\n",
    "\n",
    "            self._deltaW += np.multiply(derivative_value, error)\n",
    "            de = np.matmul(error, self._dense_weight)\n",
    "            return de\n",
    "\n",
    "        def print_info(self, input_shape: tuple):\n",
    "            super().print_info()\n",
    "            print(f\"Layer name : Dense\")\n",
    "            print(f\"Output shape: (None, {self._unit_count})\")\n",
    "            parameter_count = (input_shape[0] * self._unit_count)+ 1\n",
    "            print(f\"Parameter count: {parameter_count}\")\n",
    "            print(\"#################################\")\n",
    "            return self._unit_count, parameter_count\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._dense(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            \"\"\"\n",
    "            Indicate and perform the update weight and bias on the model.\n",
    "            \"\"\"\n",
    "            for i in range(self._unit_count):\n",
    "                self._dense_weight[i] -= learning_rate * self._deltaW[i] * self._weights\n",
    "\n",
    "            self._bias -= learning_rate * self._deltaW\n",
    "            self._deltaW = np.zeros(self._unit_count)\n",
    "\n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "            self._weights = None\n",
    "\n",
    "        def _flatten(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            self._weights = input_matrix\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        # def print_info(self, input_shape: tuple):\n",
    "        #     super().print_info()\n",
    "        #     print(f\"Output shape: (1, 1, {input_shape[0] * input_shape[1] * input_shape[2]})\")\n",
    "        #     print(f\"Parameter count: 0\\n\")\n",
    "        #     return 1, 1, input_shape[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._flatten(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            k, w, h = self._weights.shape\n",
    "            return gradient.reshape(k, w, h)\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            \"\"\"\n",
    "            Update the filter weights of the flatten layer.\n",
    "\n",
    "            Because the flatten layer has no trainable weights, this method\n",
    "            exist for the purpose of iteratively updating the weights of all\n",
    "            the layers in the model. In practice, this method does nothing.\n",
    "\n",
    "            :param learning_rate: A float specifying the learning rate of the model.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "            case \"lstm\":\n",
    "                self._layers.append(self.LSTMLayer(**kwargs))\n",
    "\n",
    "    def print_info(self, input_shape: tuple):\n",
    "        total_param = 0\n",
    "        for layer in self._layers:\n",
    "            input_shape = layer.print_info(input_shape)\n",
    "            total_param += input_shape[1]\n",
    "        print(f\"Total params: {total_param}\")\n",
    "        print(f\"Trainable params: {total_param}\")\n",
    "        print(f\"Non- trainable params: 0\")\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.forward_propagate(tensor)\n",
    "        self._forward_result = tensor\n",
    "\n",
    "    def backward_propagate(self, gradient: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        for layer in reversed(self._layers):\n",
    "            gradient = layer.backward_propagate(gradient)\n",
    "        self._backward_result = gradient\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        tensor: npt.NDArray[npt.NDArray],\n",
    "        target: npt.NDArray,\n",
    "        epochs: int = 1,\n",
    "        batch_size: int = 5,\n",
    "        learning_rate: float = 0.01,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Fit and train the CNN model.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        :param target: An ndarray of representations of the target pictures to\n",
    "                       be fed into the model.\n",
    "        :param epochs: An integer specifying the number of training epochs.\n",
    "        :param batch_size: An integer specifying the number of training batch.\n",
    "        :param learning_rate: A float specifying the learning rate of the model.\n",
    "        \"\"\"\n",
    "        out = np.array([])\n",
    "        y_target = np.array([])\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            print(\"Epoch : \", epoch + 1)\n",
    "            for i in range(len(tensor)):\n",
    "                self.forward_propagate(tensor[i])\n",
    "                forward_result = self._forward_result\n",
    "                curr_target = target[i]\n",
    "                curr_output = forward_result[0]\n",
    "                de = np.array([curr_target - curr_output]) * -1\n",
    "                self.backward_propagate(de)\n",
    "                loss += 0.5 * (curr_target - curr_output) ** 2\n",
    "                out = np.rint(np.append(out, curr_output))\n",
    "                y_target = np.append(y_target, curr_target)\n",
    "\n",
    "                if (i + 1) % batch_size == 0:\n",
    "                    for layer in reversed(self._layers):\n",
    "                        layer.update_weight(learning_rate)\n",
    "\n",
    "            avg_loss = loss / len(tensor)\n",
    "            print(\"Loss: \", avg_loss)\n",
    "            print(\"Accuracy: \", metrics.accuracy_score(y_target, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result\n",
    "\n",
    "We shall test the model we have built above using a subset of the dataset provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fb427e45bb865f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T16:14:42.704178700Z",
     "start_time": "2023-10-07T16:07:45.917263500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on dense layer...\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "Performing backward propagation on convolution layer...\n",
      "Loss:  0.15\n",
      "Accuracy:  0.7\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path).reshape((100, 1, 256, 256))[:10]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.train(image_matrix, class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc84bba76707fff2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8bf9914fe51011a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T16:20:23.934632900Z",
     "start_time": "2023-10-07T16:20:18.796816900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Utils.save_model(model, file_name=\"model.json\")\n",
    "loaded_model = Utils.load_model(\"model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a425ac8c4fc25d4a",
   "metadata": {},
   "source": [
    "### Read Dataset and Convert It Into Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5037a6628e7abff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>07-09-1984</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.500</td>\n",
       "      <td>7900</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-09-1984</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.250</td>\n",
       "      <td>600</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-09-1984</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.250</td>\n",
       "      <td>3500</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-09-1984</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.500</td>\n",
       "      <td>700</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13-09-1984</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.500</td>\n",
       "      <td>1700</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>06-12-2022</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.800</td>\n",
       "      <td>22400</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9641</th>\n",
       "      <td>07-12-2022</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.750</td>\n",
       "      <td>18000</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9642</th>\n",
       "      <td>08-12-2022</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.820</td>\n",
       "      <td>51600</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9643</th>\n",
       "      <td>09-12-2022</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.930</td>\n",
       "      <td>7800</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9644</th>\n",
       "      <td>12-12-2022</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.874</td>\n",
       "      <td>11489</td>\n",
       "      <td>3.88</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9645 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date   Low   Open  Volume  High  Close  Adjusted Close\n",
       "0     07-09-1984  5.25  5.500    7900  5.50   5.25            5.25\n",
       "1     10-09-1984  5.25  5.250     600  5.50   5.25            5.25\n",
       "2     11-09-1984  5.25  5.250    3500  5.50   5.25            5.25\n",
       "3     12-09-1984  5.50  5.500     700  5.50   5.50            5.50\n",
       "4     13-09-1984  5.00  5.500    1700  5.50   5.00            5.00\n",
       "...          ...   ...    ...     ...   ...    ...             ...\n",
       "9640  06-12-2022  3.76  3.800   22400  3.99   3.81            3.81\n",
       "9641  07-12-2022  3.68  3.750   18000  3.85   3.74            3.74\n",
       "9642  08-12-2022  3.80  3.820   51600  4.00   3.85            3.85\n",
       "9643  09-12-2022  3.85  3.930    7800  3.93   3.88            3.88\n",
       "9644  12-12-2022  3.85  3.874   11489  3.88   3.85            3.85\n",
       "\n",
       "[9645 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset/Train_stock_market.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae69f280cd615cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.78257621e-01, 2.71151896e-01, 8.28982922e-05, 2.28546793e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 6.29607282e-06, 2.28546793e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 3.67270915e-05, 2.28546793e-01,\n",
       "         3.20480404e-01],\n",
       "        ...,\n",
       "        [3.41027540e-01, 2.32925595e-01, 5.87633464e-05, 2.06985775e-01,\n",
       "         2.88874842e-01],\n",
       "        [3.59642581e-01, 2.45667695e-01, 3.14803641e-06, 2.06985775e-01,\n",
       "         3.04677623e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 1.43760329e-04, 2.17766284e-01,\n",
       "         3.20480404e-01]],\n",
       "\n",
       "       [[3.78257621e-01, 2.58409796e-01, 6.29607282e-06, 2.28546793e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 3.67270915e-05, 2.28546793e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.96872662e-01, 2.71151896e-01, 7.34541830e-06, 2.28546793e-01,\n",
       "         3.36283186e-01],\n",
       "        ...,\n",
       "        [3.59642581e-01, 2.45667695e-01, 3.14803641e-06, 2.06985775e-01,\n",
       "         3.04677623e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 1.43760329e-04, 2.17766284e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 3.77764369e-05, 2.28546793e-01,\n",
       "         3.36283186e-01]],\n",
       "\n",
       "       [[3.78257621e-01, 2.58409796e-01, 3.67270915e-05, 2.28546793e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.96872662e-01, 2.71151896e-01, 7.34541830e-06, 2.28546793e-01,\n",
       "         3.36283186e-01],\n",
       "        [3.59642581e-01, 2.71151896e-01, 1.78388730e-05, 2.28546793e-01,\n",
       "         3.04677623e-01],\n",
       "        ...,\n",
       "        [3.78257621e-01, 2.58409796e-01, 1.43760329e-04, 2.17766284e-01,\n",
       "         3.20480404e-01],\n",
       "        [3.78257621e-01, 2.58409796e-01, 3.77764369e-05, 2.28546793e-01,\n",
       "         3.36283186e-01],\n",
       "        [4.15487702e-01, 2.83893997e-01, 1.62648548e-04, 2.50107812e-01,\n",
       "         3.52085967e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.72524186e-01, 1.97247724e-01, 2.42398804e-04, 1.68175938e-01,\n",
       "         2.45891287e-01],\n",
       "        [2.86671621e-01, 1.97757395e-01, 2.21411894e-04, 1.71194488e-01,\n",
       "         2.52212394e-01],\n",
       "        [2.82948616e-01, 2.02344559e-01, 1.56352475e-04, 1.70332048e-01,\n",
       "         2.41466498e-01],\n",
       "        ...,\n",
       "        [2.66567379e-01, 1.85524978e-01, 1.99375639e-04, 1.60845199e-01,\n",
       "         2.29456380e-01],\n",
       "        [2.67311980e-01, 1.84505611e-01, 2.35053385e-04, 1.63432519e-01,\n",
       "         2.29456380e-01],\n",
       "        [2.61355172e-01, 1.81957193e-01, 1.88882185e-04, 1.57395429e-01,\n",
       "         2.25031606e-01]],\n",
       "\n",
       "       [[2.86671621e-01, 1.97757395e-01, 2.21411894e-04, 1.71194488e-01,\n",
       "         2.52212394e-01],\n",
       "        [2.82948616e-01, 2.02344559e-01, 1.56352475e-04, 1.70332048e-01,\n",
       "         2.41466498e-01],\n",
       "        [2.57632168e-01, 1.90621825e-01, 1.95178258e-04, 1.63432519e-01,\n",
       "         2.18078389e-01],\n",
       "        ...,\n",
       "        [2.67311980e-01, 1.84505611e-01, 2.35053385e-04, 1.63432519e-01,\n",
       "         2.29456380e-01],\n",
       "        [2.61355172e-01, 1.81957193e-01, 1.88882185e-04, 1.57395429e-01,\n",
       "         2.25031606e-01],\n",
       "        [2.70290383e-01, 1.85524978e-01, 5.41462263e-04, 1.63863739e-01,\n",
       "         2.31984823e-01]],\n",
       "\n",
       "       [[2.82948616e-01, 2.02344559e-01, 1.56352475e-04, 1.70332048e-01,\n",
       "         2.41466498e-01],\n",
       "        [2.57632168e-01, 1.90621825e-01, 1.95178258e-04, 1.63432519e-01,\n",
       "         2.18078389e-01],\n",
       "        [2.39017127e-01, 1.69724776e-01, 4.34429025e-04, 1.47046140e-01,\n",
       "         2.08596714e-01],\n",
       "        ...,\n",
       "        [2.61355172e-01, 1.81957193e-01, 1.88882185e-04, 1.57395429e-01,\n",
       "         2.25031606e-01],\n",
       "        [2.70290383e-01, 1.85524978e-01, 5.41462263e-04, 1.63863739e-01,\n",
       "         2.31984823e-01],\n",
       "        [2.74013388e-01, 1.91131509e-01, 8.18489467e-05, 1.60845199e-01,\n",
       "         2.33881170e-01]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data_train = scaler.fit_transform(data[[\"Low\", \"Open\", \"Volume\", \"High\", \"Close\"]].values)\n",
    "x_train, y_train = Utils.create_sequences(data_train, 10)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6761c127ed3779fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer LSTM\n",
      "Layer name : lstm\n",
      "Output shape: (None, 10)\n",
      "Parameter count: 480\n",
      "#################################\n",
      "Layer dense\n",
      "Layer name : Dense\n",
      "Output shape: (None, 1)\n",
      "Parameter count: 11\n",
      "#################################\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non- trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add_layer(\"lstm\", cell_count=10, input_shape=(2, 1))\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"relu\")\n",
    "model.print_info((2, 1))\n",
    "# model = model.LSTMLayer(cell_count=10, input_shape=(x_train.shape[1], 1))\n",
    "# lstm_result = model.forward_propagate(x_train)\n",
    "# print(lstm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e37158191d82d24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T09:12:02.367754500Z",
     "start_time": "2023-11-03T09:12:02.345528300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on LSTM layer...\n",
      "\n",
      "[[0.52826613]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[[1, 2]]])\n",
    "model = Model()\n",
    "model = model.LSTMLayer(cell_count=1, input_shape=(data.shape[1], data.shape[2]))\n",
    "lstm_result = model.forward_propagate(data)\n",
    "print(lstm_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
