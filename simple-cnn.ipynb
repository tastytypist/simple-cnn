{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fade326f",
   "metadata": {},
   "source": [
    "# Tugas Besar IF4074 - Pembelajaran Mesin Lanjut\n",
    "# Implementasi Convolutional Neural Network\n",
    "\n",
    "# Simple CNN\n",
    "**Simple CNN** is a convolutional neural network implemented in Python and fine-tuned using backpropagation algorithm.\n",
    "\n",
    "## Setup\n",
    "Assuming you've installed the latest version of Python (if not, guides for it are widely available),\n",
    "1. ensure pip is installed by running `python -m ensurepip --upgrade`;\n",
    "2. install the Python dependencies by running `pip install -r requirements.txt`.\n",
    "\n",
    "## Contribution (Milestone 1)\n",
    "| NIM      | Name                   | Contribution(s)                                                       |\n",
    "|----------|------------------------|-----------------------------------------------------------------------|\n",
    "| 13520041 | Ilham Pratama          | Dataset handling; Detector, Pooling, Dense, and Flatten layer; Report |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | Class model; Convolutional layer; Report                              |\n",
    "\n",
    "## Contribution (Milestone 2)\n",
    "| NIM      | Name                   | Contribution(s) |\n",
    "|----------|------------------------|-----------------|\n",
    "| 13520041 | Ilham Pratama          | -               |\n",
    "| 13520042 | Jeremy S.O.N. Simbolon | -               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642a8a3",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-07T15:16:10.840508200Z",
     "start_time": "2023-10-07T15:16:09.601581700Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "import cv2\n",
    "import jsonpickle\n",
    "import jsonpickle.ext.numpy\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "from scipy.special import expit\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad81320",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5d02ba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T15:16:13.348072800Z",
     "start_time": "2023-10-07T15:16:13.342069Z"
    }
   },
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Module related utility functions.\n",
    "\n",
    "    This class is used to prepare the image dataset for the CNN model. In\n",
    "    addition, this class is also used to save and load the CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_path: str) -> tuple[npt.NDArray, npt.NDArray, dict]:\n",
    "        \"\"\"\n",
    "        Preprocess the dataset and return useful information for further processing.\n",
    "\n",
    "        :param dataset_path: A string representation of the path pointing to\n",
    "                             the dataset.\n",
    "        :return: A tuple consisted of an ndarray of dataset image path, an\n",
    "                 ndarray of image labels, and a dictionary that maps class\n",
    "                 labels to folder name.\n",
    "        \"\"\"\n",
    "        folder_list = sorted(os.listdir(dataset_path))\n",
    "        image_path = []\n",
    "        image_label = np.array([], dtype=np.int16)\n",
    "        image_dictionary = {}\n",
    "        for i, folder_name in enumerate(folder_list):\n",
    "            class_folder_path = os.path.join(dataset_path, folder_name)\n",
    "            list_image_name = sorted(os.listdir(class_folder_path))\n",
    "            temp_folder_path = [os.path.join(class_folder_path, image_name) for image_name in list_image_name]\n",
    "\n",
    "            image_path += temp_folder_path\n",
    "            temp_class_label = np.full(len(list_image_name), i, dtype=np.int16)\n",
    "            image_label = np.concatenate((image_label, temp_class_label), axis=0)\n",
    "            image_dictionary[str(i)] = folder_name\n",
    "\n",
    "        return np.asarray(image_path), image_label, image_dictionary\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_image_to_matrix(path: npt.NDArray) -> npt.NDArray:\n",
    "        \"\"\"\n",
    "        Convert the image dataset into a list of ndarray.\n",
    "\n",
    "        Each ndarray is an RGB representation of each image in the dataset.\n",
    "\n",
    "        :param path: An ndarray of string representation of the path pointing\n",
    "                     to each image entry in the dataset.\n",
    "        :return: A list of ndarray representation of the image in the dataset.\n",
    "        \"\"\"\n",
    "        list_of_image_matrix = []\n",
    "        size = (256, 256)\n",
    "\n",
    "        for file_img in path:\n",
    "            image = cv2.imread(file_img, 1)\n",
    "            matrix = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            matrix = cv2.resize(matrix, size)\n",
    "            list_of_image_matrix.append(matrix)\n",
    "\n",
    "        return np.array(list_of_image_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(model_object: \"Model\", file_name: str = \"model.json\") -> None:\n",
    "        \"\"\"\n",
    "        Save the specified model into a JSON file.\n",
    "\n",
    "        :param model_object: The model to be saved.\n",
    "        :param file_name: A string specifying the file name of the saved model.\n",
    "        \"\"\"\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"w\") as file:\n",
    "            json = jsonpickle.encode(model_object, indent=4)\n",
    "            file.write(json)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_name: str = \"model.json\") -> \"Model\":\n",
    "        \"\"\"\n",
    "        Load a model from the specified JSON file name.\n",
    "\n",
    "        :param file_name: A string specifying the file name of the model to be\n",
    "                          loaded.\n",
    "        :return: The loaded model from the specified file.\n",
    "        \"\"\"\n",
    "        jsonpickle.ext.numpy.register_handlers()\n",
    "        with open(file_name, \"r\") as file:\n",
    "            json = file.read()\n",
    "            return jsonpickle.decode(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f0f7",
   "metadata": {},
   "source": [
    "### Model Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9981df315bc56c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T15:16:17.677692900Z",
     "start_time": "2023-10-07T15:16:17.500231700Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    The convolutional neural network model used to classify images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Instantiate the convolutional neural network model.\n",
    "        \"\"\"\n",
    "        self._layers = []\n",
    "        self._forward_result = None\n",
    "        self._backward_result = None\n",
    "\n",
    "    class Layer:\n",
    "        \"\"\"\n",
    "        Base representation of the layer used as part of the convolutional\n",
    "        neural network architecture.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, name) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the base layer.\n",
    "\n",
    "            :param name: Name of the layer.\n",
    "            \"\"\"\n",
    "            self._name = name\n",
    "\n",
    "        def forward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the forward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing forward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "        def backward_propagate(self) -> None:\n",
    "            \"\"\"Indicate the backward propagation is being performed.\"\"\"\n",
    "            print(f\"Performing backward propagation on {self._name} layer...\")\n",
    "            print()\n",
    "\n",
    "    class ConvolutionLayer(Layer):\n",
    "        \"\"\"\n",
    "        The convolutional layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used\n",
    "        to perform the convolution operation on the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            filter_count: int,\n",
    "            filter_size: tuple[int, int] = (32, 32),\n",
    "            padding_size: int = 0,\n",
    "            stride_size: tuple[int, int] = (1, 1),\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the convolutional layer.\n",
    "\n",
    "            :param filter_count: An integer specifying the amount of feature\n",
    "                                 to be extracted in the form of the amount of\n",
    "                                 filters.\n",
    "            :param filter_size: A tuple of two integers specifying the height\n",
    "                                and width of the convolution filter.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param stride_size: A tuple of two integers specifying the pixel\n",
    "                                step size along the height and width of the\n",
    "                                input weight.\n",
    "            \"\"\"\n",
    "            super().__init__(\"convolution\")\n",
    "            self._filter_count = filter_count\n",
    "            self._filter_dimension = 0\n",
    "            self._filter_height, self._filter_width = filter_size\n",
    "            self._filter_weights = None\n",
    "            self._padding_size = padding_size\n",
    "            self._stride_height, self._stride_width = stride_size\n",
    "            self._output_height = 0\n",
    "            self._output_width = 0\n",
    "            self._weight_dimension = 0\n",
    "            self._weight_height = 0\n",
    "            self._weight_width = 0\n",
    "            self._weights = None\n",
    "            self._biases = None\n",
    "            self._filter_gradients = []\n",
    "            self._bias_gradients = []\n",
    "\n",
    "        def _pad_weights(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "            padding_size: int,\n",
    "            forward: bool = True\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Pad the specified weights with 0's around it.\n",
    "\n",
    "            :param weights: The ndarray of weights to be padded with 0's.\n",
    "            :param padding_size: An integer specifying the dimension of 0's to\n",
    "                                 be added around the weight.\n",
    "            :param forward: A boolean specifying whether the padding is \n",
    "                            performed during forward propagation.\n",
    "            :return: An ndarray of weights padded with 0's.\n",
    "            \"\"\"\n",
    "            weight_dimension = len(weights)\n",
    "            weight_height = len(weights[0])\n",
    "            weight_width = len(weights[0][0])\n",
    "            \n",
    "            if forward:\n",
    "                self._weight_dimension = weight_dimension\n",
    "                self._weight_height = weight_height + 2 * padding_size\n",
    "                self._weight_width = weight_width + 2 * padding_size\n",
    "\n",
    "            padded_weights = [\n",
    "                [\n",
    "                    [\n",
    "                        0.0\n",
    "                        if k < padding_size or k >= weight_width + padding_size\n",
    "                        or j < padding_size or j >= weight_height + padding_size\n",
    "                        else weights[i][j - padding_size][k - padding_size]\n",
    "                        for k in range(weight_width + 2 * padding_size)\n",
    "                    ]\n",
    "                    for j in range(weight_height + 2 * padding_size)\n",
    "                ]\n",
    "                for i in range(weight_dimension)\n",
    "            ]\n",
    "\n",
    "            return np.array(padded_weights)\n",
    "\n",
    "        def _convolute(\n",
    "            self,\n",
    "            weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]],\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Perform the convolution operation on the input weights.\n",
    "\n",
    "            :param weights: An ndarray of input weights.\n",
    "            :return: An ndarray of features extracted from the weights.\n",
    "            \"\"\"\n",
    "            self._weights = np.array(weights)\n",
    "            self._filter_dimension = len(weights)\n",
    "            self._output_height = (\n",
    "                math.ceil((len(weights[0]) - self._filter_height + 2 * self._padding_size) / self._stride_height) + 1\n",
    "            )\n",
    "            self._output_width = (\n",
    "                math.ceil((len(weights[0][0]) - self._filter_width + 2 * self._padding_size) / self._stride_width) + 1\n",
    "            )\n",
    "\n",
    "            if self._filter_weights is None:\n",
    "                self._filter_weights = np.random.rand(\n",
    "                    self._filter_count,\n",
    "                    self._filter_dimension,\n",
    "                    self._filter_height,\n",
    "                    self._filter_width,\n",
    "                )\n",
    "            if self._biases is None:\n",
    "                self._biases = np.random.rand(self._filter_count, self._output_height, self._output_width)\n",
    "\n",
    "            feature_maps = np.copy(self._biases)\n",
    "            weights = self._pad_weights(weights, self._padding_size)\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(0, self._weight_height - self._filter_height + 1, self._stride_height):\n",
    "                    for k in range(0, self._weight_width - self._filter_width + 1, self._stride_width):\n",
    "                        for l in range(self._filter_dimension):\n",
    "                            field = weights[l, j : j + self._filter_height, k : k + self._filter_width]\n",
    "                            feature = field * self._filter_weights[i][l]\n",
    "                            feature_maps[i][j][k] += np.sum(feature)\n",
    "            return feature_maps\n",
    "\n",
    "        def _calculate_gradient(self, output_gradient: npt.NDArray) -> npt.NDArray:\n",
    "            output_gradient_height = len(output_gradient[0])\n",
    "            output_gradient_width = len(output_gradient[0][0])\n",
    "            \n",
    "            filter_gradient = np.zeros((self._filter_count, self._filter_dimension, self._filter_height, self._filter_width))\n",
    "            input_gradient = np.zeros((self._weight_dimension, self._weight_height, self._weight_width))\n",
    "            padded_output_gradient = self._pad_weights(output_gradient, 2, forward=False)\n",
    "\n",
    "            for i in range(self._filter_count):\n",
    "                for j in range(self._filter_dimension):\n",
    "                    for k in range(0, self._weight_height - output_gradient_height + 1, self._stride_height):\n",
    "                        for l in range(0, self._weight_width - output_gradient_width + 1, self._stride_width):\n",
    "                            field = self._weights[j, k : k + output_gradient_height, l : l + output_gradient_width]\n",
    "                            gradient = field * output_gradient[i]\n",
    "                            filter_gradient[i][j][k][l] = np.sum(gradient)\n",
    "                    for k in range(0, output_gradient_height - self._filter_height + 1, self._stride_height):\n",
    "                        for l in range(0, output_gradient_width - self._filter_width + 1, self._stride_width):\n",
    "                            field = padded_output_gradient[i, k : k + self._filter_height, l : l + self._filter_width]\n",
    "                            gradient = field * np.rot90(self._filter_weights[i][j], k=2)\n",
    "                            input_gradient[j][k][l] += np.sum(gradient)\n",
    "            \n",
    "            self._filter_gradients.append(filter_gradient)\n",
    "            self._bias_gradients.append(output_gradient)\n",
    "            \n",
    "            return input_gradient\n",
    "\n",
    "        def forward_propagate(\n",
    "            self, weights: npt.NDArray[npt.NDArray[npt.NDArray[float]]]\n",
    "        ) -> npt.NDArray[npt.NDArray[npt.NDArray[float]]]:\n",
    "            \"\"\"\n",
    "            Indicate and perform the convolution process on the input weights.\n",
    "\n",
    "            :param weights: The ndarray of weights to be convoluted.\n",
    "            :return: An ndarray of convoluted weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._convolute(weights)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            self._filter_weights -= learning_rate * np.average(np.array(self._filter_gradients), axis=0)\n",
    "            self._biases -= learning_rate * np.average(np.array(self._bias_gradients), axis=0)\n",
    "            self._filter_gradients = []\n",
    "            self._bias_gradients = []\n",
    "\n",
    "    class DetectorLayer(Layer):\n",
    "        \"\"\"\n",
    "        The detector layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        introduce non-linearity to the learning process using the reLU\n",
    "        activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the detector layer.\"\"\"\n",
    "            super().__init__(\"detector\")\n",
    "            self._weights = None\n",
    "\n",
    "        def _detect(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Apply the reLU activation function on the input weights.\n",
    "\n",
    "            :param feature: An ndarray of input weights.\n",
    "            :return: An ndarray of weights on which the reLU function has been\n",
    "                     applied.\n",
    "            \"\"\"\n",
    "            self._weights = feature\n",
    "            return np.maximum(feature, 0)\n",
    "\n",
    "        def _calculate_gradient(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the detector layer.\n",
    "            Use reLu derivative: dreLU/dx = 1 if x > 0, otherwise 0.\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            dx = error * (self._weights > 0)\n",
    "            return dx\n",
    "\n",
    "        def forward_propagate(self, feature: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the detector process on the input weights.\n",
    "\n",
    "            :param feature: The ndarray of weights on which reLU function is\n",
    "                            to be applied.\n",
    "            :return: An ndarray of activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._detect(feature)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            pass\n",
    "\n",
    "    class PoolingLayer(Layer):\n",
    "        \"\"\"\n",
    "        The pooling layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        down-sample the input weights according to the specified pooling\n",
    "        operation.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, filter_size: int, stride_size: int, mode: str = \"max\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the pooling layer.\n",
    "\n",
    "            :param filter_size: An integer specifying the dimension of the\n",
    "                                pooling window.\n",
    "            :param stride_size: An integer specifying the pixel step size along\n",
    "                                the height and width of the input weight.\n",
    "            :param mode: A string specifying the preferred pooling operation.\n",
    "                         Must either be ``average`` or ``max``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"pooling\")\n",
    "            self._filter_size = filter_size\n",
    "            self._stride_size = stride_size\n",
    "            self._mode = mode\n",
    "            self._weights = None\n",
    "\n",
    "        def _average(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the average of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The average of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.average(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def _max(self, input_matrix: npt.NDArray, d: int, h: int, w: int) -> float:\n",
    "            \"\"\"\n",
    "            Take the maximum of the input values over the pooling window.\n",
    "\n",
    "            :param input_matrix: The ndarray of weights on which the operation\n",
    "                                 is applied.\n",
    "            :param d: An integer specifying the depth location of the pooling\n",
    "                      window.\n",
    "            :param h: An integer specifying the height location of the pooling\n",
    "                      window.\n",
    "            :param w: An integer specifying the width location of the pooling\n",
    "                      window.\n",
    "            :return: The maximum of the input values.\n",
    "            \"\"\"\n",
    "            h_start = h * self._stride_size\n",
    "            w_start = w * self._stride_size\n",
    "            h_end = h_start + self._filter_size\n",
    "            w_end = w_start + self._filter_size\n",
    "            return np.max(input_matrix[d, h_start:h_end, w_start:w_end])\n",
    "\n",
    "        def _pool(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            self._weights = input_matrix\n",
    "            depth, height, width = input_matrix.shape\n",
    "            filter_height = (height - self._filter_size) // self._stride_size + 1\n",
    "            filter_width = (width - self._filter_size) // self._stride_size + 1\n",
    "            pooled = np.zeros([depth, filter_height, filter_width], dtype=np.double)\n",
    "            for d in range(0, depth):\n",
    "                for h in range(0, filter_height):\n",
    "                    for w in range(0, filter_width):\n",
    "                        if self._mode == \"average\":\n",
    "                            pooled[d, h, w] = self._average(input_matrix, d, h, w)\n",
    "                        elif self._mode == \"max\":\n",
    "                            pooled[d, h, w] = self._max(input_matrix, d, h, w)\n",
    "            return pooled\n",
    "\n",
    "        def _calculate_gradient(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            f, w, h = self._weights.shape\n",
    "            dx = np.zeros(self._weights.shape)\n",
    "            for i in range(0, f):\n",
    "                for j in range(0, w, self._filter_size):\n",
    "                    for k in range(0, h, self._filter_size):\n",
    "                        input_slice = self._weights[i, j: j + self._filter_size, k: k + self._filter_size]\n",
    "                        max_input_slice = np.argmax(input_slice)\n",
    "                        max_idx = np.unravel_index(max_input_slice, (self._filter_size, self._filter_size))\n",
    "                        if (j + max_idx[0]) < w and (k + max_idx[1]) < h:\n",
    "                            dx[i, j + max_idx[0], k + max_idx[1]] = error[\n",
    "                                i, int(j // self._filter_size), int(k // self._filter_size)\n",
    "                            ]\n",
    "            return dx\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the pooling operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of down-sampled weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._pool(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            pass\n",
    "\n",
    "    class DenseLayer(Layer):\n",
    "        \"\"\"\n",
    "        The dense layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        abstractly represent the input data using its weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, unit_count: int, activation: str = \"sigmoid\") -> None:\n",
    "            \"\"\"\n",
    "            Instantiate the dense layer.\n",
    "\n",
    "            :param unit_count: An integer specifying the dimension of the\n",
    "                               output space.\n",
    "            :param activation: The activation function to be applied to each\n",
    "                               node. Must either be ``sigmoid`` or ``relu``.\n",
    "            \"\"\"\n",
    "            super().__init__(\"dense\")\n",
    "            self._unit_count = unit_count\n",
    "            self._activation = activation\n",
    "            self._bias = np.zeros(unit_count)\n",
    "            self._dense_weight = []\n",
    "            self._weights = None\n",
    "            self._output = 0.0\n",
    "            self._deltaW = np.zeros(unit_count)\n",
    "\n",
    "        @staticmethod\n",
    "        def _sigmoid_derivative(input_: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Take derivative value from input.\n",
    "            \"\"\"\n",
    "            sigmoid = 1 / (1 + np.exp(-input_))\n",
    "            return sigmoid * (1 - sigmoid)\n",
    "\n",
    "        @staticmethod\n",
    "        def _relu_derivative(input_: npt.NDArray) -> int:\n",
    "            \"\"\"\n",
    "            Take derivative value from input.\n",
    "            \"\"\"\n",
    "            if input_ >= 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        def _derivative_act_func(self, activation: str, input_: npt.NDArray) -> npt.NDArray | float:\n",
    "            \"\"\"\n",
    "            Take derivative value from activation function and input.\n",
    "            \"\"\"\n",
    "            if activation == \"sigmoid\":\n",
    "                return self._sigmoid_derivative(input_)\n",
    "            else:\n",
    "                return self._relu_derivative(input_)\n",
    "\n",
    "        def _dense(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the linear combination and activation of the input weights\n",
    "            using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            self._weights = input_matrix\n",
    "            if len(self._dense_weight) == 0:\n",
    "                self._dense_weight = np.random.randn(self._unit_count, len(self._weights))\n",
    "            result = np.zeros(self._unit_count)\n",
    "\n",
    "            for i in range(self._unit_count):\n",
    "                input_weight = np.sum(self._dense_weight[i] * input_matrix)\n",
    "                result[i] = input_weight + self._bias[i]\n",
    "\n",
    "            if self._activation == \"sigmoid\":\n",
    "                self.output = expit(result)\n",
    "            elif self._activation == \"relu\":\n",
    "                self.output = np.maximum(result, 0)\n",
    "            return self.output\n",
    "\n",
    "        def _calculate_gradient(self, error: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the backward propagation on the layer.\n",
    "\n",
    "            :param error: The gradient from the next layer.\n",
    "            :return: The gradient for the previous layer.\n",
    "            \"\"\"\n",
    "            derivative_value = np.array([])\n",
    "            for i in self.output:\n",
    "                derivative_value = np.append(derivative_value, self._derivative_act_func(self._activation, i))\n",
    "\n",
    "            self._deltaW += np.multiply(derivative_value, error)\n",
    "            de = np.matmul(error, self._dense_weight)\n",
    "            return de\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the linear combination and activation of the\n",
    "            input weights using the layer's weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of linearly-combined and activated weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._dense(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            output_gradient = self._calculate_gradient(gradient)\n",
    "            return output_gradient\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            \"\"\"\n",
    "            Indicate and perform the update weight and bias on the model\n",
    "            \"\"\"\n",
    "            for i in range(self._unit_count):\n",
    "                self._dense_weight[i] -= learning_rate * self._deltaW[i] * self._weights\n",
    "\n",
    "            self._bias -= learning_rate * self._deltaW\n",
    "            self._deltaW = np.zeros(self._unit_count)\n",
    "\n",
    "    class FlattenLayer(Layer):\n",
    "        \"\"\"\n",
    "        The flatten layer in convolutional neural network.\n",
    "\n",
    "        This class is inherited from the ``Layer`` class. This layer is used to\n",
    "        flatten the input weights.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self) -> None:\n",
    "            \"\"\"Instantiate the flatten layer.\"\"\"\n",
    "            super().__init__(\"flatten\")\n",
    "            self._weights = None\n",
    "\n",
    "        def _flatten(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            self._weights = input_matrix\n",
    "            return input_matrix.flatten()\n",
    "\n",
    "        def forward_propagate(self, input_matrix: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the flatten operation on the input weights.\n",
    "\n",
    "            :param input_matrix: An ndarray of input weights.\n",
    "            :return: An ndarray of flatten weights.\n",
    "            \"\"\"\n",
    "            super().forward_propagate()\n",
    "            result = self._flatten(input_matrix)\n",
    "            return result\n",
    "\n",
    "        def backward_propagate(self, gradient: npt.NDArray) -> npt.NDArray:\n",
    "            \"\"\"\n",
    "            Indicate and perform the backward propagation operation on the model.\n",
    "            \"\"\"\n",
    "            super().backward_propagate()\n",
    "            k, w, h = self._weights.shape\n",
    "            return gradient.reshape(k, w, h)\n",
    "\n",
    "        def update_weight(self, learning_rate: float) -> None:\n",
    "            pass\n",
    "\n",
    "    def add_layer(self, name: str, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Sequentially add the specified layer into the model.\n",
    "\n",
    "        :param name: A string representation of the layer to be added.\n",
    "        :param kwargs: Layer-related parameters in the form of key-value pairs.\n",
    "        \"\"\"\n",
    "        match name:\n",
    "            case \"convolution\":\n",
    "                self._layers.append(self.ConvolutionLayer(**kwargs))\n",
    "            case \"detector\":\n",
    "                self._layers.append(self.DetectorLayer())\n",
    "            case \"pooling\":\n",
    "                self._layers.append(self.PoolingLayer(**kwargs))\n",
    "            case \"dense\":\n",
    "                self._layers.append(self.DenseLayer(**kwargs))\n",
    "            case \"flatten\":\n",
    "                self._layers.append(self.FlattenLayer())\n",
    "\n",
    "    def forward_propagate(self, tensor: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the forward propagation operation on the model.\n",
    "\n",
    "        :param tensor: An ndarray of input weights representing the input\n",
    "                       pictures.\n",
    "        \"\"\"\n",
    "        for layer in self._layers:\n",
    "            tensor = layer.forward_propagate(tensor)\n",
    "        self._forward_result = tensor\n",
    "\n",
    "    def backward_propagate(self, gradient: npt.NDArray) -> None:\n",
    "        \"\"\"\n",
    "        Indicate and perform the backward propagation operation on the model.\n",
    "        \"\"\"\n",
    "        for layer in reversed(self._layers):\n",
    "            gradient = layer.backward_propagate(gradient)\n",
    "        self._backward_result = gradient\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        tensor: npt.NDArray[npt.NDArray],\n",
    "        target: npt.NDArray,\n",
    "        epochs: int = 1,\n",
    "        batch_size: int = 5,\n",
    "        learning_rate: float = 0.01,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Fit and train the CNN model.\n",
    "\n",
    "        :param tensor: An ndarray of representations of the input pictures to\n",
    "                       be fed into the model.\n",
    "        :param target: An ndarray of representations of the target pictures to\n",
    "                       be fed into the model.\n",
    "        :param epochs: An integer specifying the number of training epochs.\n",
    "        :param batch_size: An integer specifying the number of training batch.\n",
    "        :param learning_rate: A float specifying the learning rate of the model.\n",
    "        \"\"\"\n",
    "        out = np.array([])\n",
    "        y_target = np.array([])\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            print(\"Epoch : \", epoch + 1)\n",
    "            for i in range(len(tensor)):\n",
    "                self.forward_propagate(tensor[i])\n",
    "                forward_result = self._forward_result\n",
    "                curr_target = target[i]\n",
    "                curr_output = forward_result[0]\n",
    "                de = np.array([curr_target - curr_output]) * -1\n",
    "                self.backward_propagate(de)\n",
    "                loss += 0.5 * (curr_target - curr_output) ** 2\n",
    "                out = np.rint(np.append(out, curr_output))\n",
    "                y_target = np.append(y_target, curr_target)\n",
    "\n",
    "                if (i + 1) % batch_size == 0:\n",
    "                    for layer in reversed(self._layers):\n",
    "                        layer.update_weight(learning_rate)\n",
    "\n",
    "            avg_loss = loss / len(tensor)\n",
    "            print(\"Loss: \", avg_loss)\n",
    "            print(\"Accuracy: \", metrics.accuracy_score(y_target, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6dd31",
   "metadata": {},
   "source": [
    "### Test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fb427e45bb865f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T15:17:50.636128Z",
     "start_time": "2023-10-07T15:16:29.644793700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n"
     ]
    }
   ],
   "source": [
    "folder_path, class_label, class_dictionary = Utils.load_dataset(\"./dataset\")\n",
    "image_matrix = Utils.convert_image_to_matrix(folder_path)\n",
    "image_number = 0\n",
    "image_matrix = [image_matrix[image_number]]\n",
    "\n",
    "model = Model()\n",
    "model.add_layer(\n",
    "    \"convolution\",\n",
    "    filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),\n",
    ")\n",
    "model.add_layer(\"detector\")\n",
    "model.add_layer(\"pooling\", filter_size=3, stride_size=2, mode=\"average\")\n",
    "model.add_layer(\"flatten\")\n",
    "model.add_layer(\"dense\", unit_count=8, activation=\"relu\")\n",
    "model.add_layer(\"dense\", unit_count=1, activation=\"sigmoid\")\n",
    "model.forward_propagate(image_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccf9ca7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T15:19:57.986284700Z",
     "start_time": "2023-10-07T15:18:38.278338600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing forward propagation on convolution layer...\n",
      "Performing forward propagation on detector layer...\n",
      "\n",
      "Performing forward propagation on pooling layer...\n",
      "Performing forward propagation on flatten layer...\n",
      "\n",
      "Performing forward propagation on dense layer...\n",
      "\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "convLayer = model.ConvolutionLayer(filter_count=32,\n",
    "    filter_size=(3, 3),\n",
    "    padding_size=0,\n",
    "    stride_size=(1, 1),)\n",
    "\n",
    "result = convLayer.forward_propagate(image_matrix)\n",
    "detectLayer = model.DetectorLayer()\n",
    "result = detectLayer.forward_propagate(result)\n",
    "poolLayer = model.PoolingLayer(filter_size=3, stride_size=2, mode=\"average\")\n",
    "result = poolLayer.forward_propagate(result)\n",
    "flatLayer = model.FlattenLayer()\n",
    "result = flatLayer.forward_propagate(result)\n",
    "denseLayer = model.DenseLayer(unit_count=1, activation=\"sigmoid\")\n",
    "result = denseLayer.forward_propagate(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25d11e",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-07T15:20:41.961448500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing backward propagation on dense layer...\n",
      "\n",
      "[-0.10553458 -0.81807532 -0.36816373 ... -0.67281912  2.06658002\n",
      " -0.17961479]\n",
      "\n",
      "\n",
      "Performing backward propagation on flatten layer...\n",
      "\n",
      "[[[-1.05534581e-01 -8.18075320e-01 -3.68163733e-01 ... -9.29202034e-01\n",
      "    3.66774615e-01 -1.45421627e-01]\n",
      "  [ 6.83150520e-01  1.62291452e+00 -1.56877288e+00 ... -4.47359020e-01\n",
      "    5.38262857e-01 -4.20047730e-01]\n",
      "  [-1.59136919e+00 -3.29970328e-01 -9.39243876e-01 ...  1.07001934e-01\n",
      "   -3.36111352e-02 -2.00132613e-02]\n",
      "  ...\n",
      "  [ 2.42671617e-01 -1.24066163e+00 -1.44437247e+00 ...  3.04579748e-02\n",
      "    1.91927604e+00 -1.51651555e-01]\n",
      "  [-1.91831161e-01 -2.07710340e+00  1.14975418e-01 ...  9.68610451e-01\n",
      "    8.73352081e-01 -9.87574987e-01]\n",
      "  [-1.87755160e+00  1.70448794e+00 -5.31313346e-01 ...  9.08260131e-01\n",
      "   -8.97268959e-01 -2.90550942e-01]]\n",
      "\n",
      " [[ 8.79682394e-01 -7.96088607e-01  1.27326677e+00 ... -1.05473920e+00\n",
      "   -5.89419804e-01  1.19082380e+00]\n",
      "  [ 1.07185865e+00 -6.57004539e-01  3.14234708e-01 ...  9.68525211e-01\n",
      "    8.44054152e-01 -3.73211044e-01]\n",
      "  [ 3.48970009e-02  7.63048991e-04  1.41535392e+00 ... -8.11522474e-01\n",
      "    8.55133765e-01  2.31516576e-01]\n",
      "  ...\n",
      "  [-1.21821603e+00 -1.80389480e+00  2.60838395e-01 ...  2.70905002e-01\n",
      "   -3.42983482e-01  4.86789097e-01]\n",
      "  [-9.48039805e-01 -1.22618463e+00 -3.65104685e-02 ... -1.13179066e-01\n",
      "   -4.14925234e-01  5.49093961e-01]\n",
      "  [ 2.20831458e+00  1.37749170e-01  1.81382576e+00 ... -3.79770483e-01\n",
      "   -2.94423846e-01  9.90165309e-01]]\n",
      "\n",
      " [[-8.54949340e-01  1.32016079e+00 -5.95325360e-01 ... -1.11555762e+00\n",
      "    8.54640500e-01 -3.69722365e-01]\n",
      "  [ 5.14060982e-01  9.11897605e-01  8.36099937e-01 ...  2.81381498e-01\n",
      "   -4.29093554e-01 -1.79978119e+00]\n",
      "  [-1.52254244e-01  4.56918085e-01  1.96066175e-01 ...  6.77331399e-02\n",
      "   -9.29099185e-02 -4.02953779e+00]\n",
      "  ...\n",
      "  [ 7.80244130e-01 -3.78368453e-01 -5.43900471e-02 ...  1.44702216e+00\n",
      "    8.83258549e-02  5.96498289e-01]\n",
      "  [ 4.28616055e-01 -7.08466026e-01  6.94745304e-01 ... -3.39376444e-01\n",
      "   -6.00399311e-01 -1.16338596e+00]\n",
      "  [-1.60849977e+00  1.45160933e+00 -1.97660384e+00 ... -2.27870138e+00\n",
      "    9.68811983e-01 -4.78199987e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-5.50170688e-02 -6.98953141e-01  1.06832821e+00 ... -6.63110787e-01\n",
      "    1.75575332e-01  3.29383716e-01]\n",
      "  [-1.10703479e+00 -4.01717601e-01 -1.44100487e+00 ...  6.07325896e-02\n",
      "    1.09851635e+00  1.05729228e+00]\n",
      "  [-9.26218911e-01  5.52152855e-01  1.11411862e+00 ...  3.31489176e-01\n",
      "   -1.18805088e+00 -3.14018356e-02]\n",
      "  ...\n",
      "  [-2.46682910e-01  1.15481225e+00  1.31613527e+00 ... -5.64218169e-01\n",
      "    9.14386521e-01 -4.24775361e-01]\n",
      "  [ 2.02232566e+00  5.12626424e-01 -3.43880470e-01 ...  1.59928170e-01\n",
      "   -7.55719339e-01 -9.66622833e-01]\n",
      "  [-9.49848052e-01 -1.31545560e+00  6.51675530e-01 ... -1.68954983e+00\n",
      "    5.38853257e-01  5.40762063e-01]]\n",
      "\n",
      " [[ 7.04353754e-01 -3.86047824e-01  7.42988273e-01 ... -1.09466428e+00\n",
      "   -2.55674794e-01 -6.70509819e-01]\n",
      "  [-5.70694194e-01  2.30707166e+00 -2.31227390e+00 ...  4.92108818e-01\n",
      "    1.40026498e+00 -1.78599594e+00]\n",
      "  [ 1.00578921e+00  1.65593514e+00 -9.91462378e-01 ... -4.28898739e-01\n",
      "    2.17630243e+00  1.11490223e+00]\n",
      "  ...\n",
      "  [-3.00940456e-01 -2.26320718e+00 -2.77530268e-01 ...  1.20894062e+00\n",
      "    7.36379661e-01  6.70459383e-01]\n",
      "  [-1.31919437e-01  3.86442340e-01  3.11304728e-01 ...  3.04519268e-01\n",
      "   -2.77858322e-01 -5.51560104e-01]\n",
      "  [ 2.07737217e-01 -3.80671630e-01 -5.51017152e-01 ... -2.04172923e+00\n",
      "   -6.82803438e-02  2.63362025e-01]]\n",
      "\n",
      " [[-2.39418480e-01  1.99205275e+00 -1.53304053e+00 ... -2.28429039e+00\n",
      "    5.24319759e-01 -1.13074346e+00]\n",
      "  [ 4.04642161e-01 -1.37222427e+00  2.26889355e-01 ...  4.71564671e-01\n",
      "    5.25549323e-01 -6.90457525e-01]\n",
      "  [-1.35927271e+00  8.84461692e-01  9.96913962e-01 ...  5.02157387e-01\n",
      "   -1.85289905e+00 -5.92561898e-01]\n",
      "  ...\n",
      "  [ 1.86054293e+00  3.07719262e-02 -2.67190954e-01 ... -8.72652598e-01\n",
      "   -3.49014162e-01  8.21736683e-01]\n",
      "  [ 1.46848590e+00  2.45539194e-01  1.07894834e+00 ... -4.34214408e-01\n",
      "    4.80599845e-01 -2.10341218e+00]\n",
      "  [-1.61542955e-01  3.31976183e-01 -2.66805001e-01 ... -6.72819115e-01\n",
      "    2.06658002e+00 -1.79614794e-01]]]\n",
      "\n",
      "\n",
      "Performing backward propagation on pooling layer...\n",
      "[[[-0.10553458  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 1.67194795  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-0.35025905  0.          0.         ...  0.          0.\n",
      "    2.31498593]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.87968239  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 1.50695895  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.95206542\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.85494934  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          2.37841846\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.47641739  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.60153491\n",
      "    0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.05501707  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-1.61362444  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.21158483\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.70435375  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.55067806\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.37763123  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          1.04015805\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.23941848  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-2.21100132  0.          0.         ...  0.          0.\n",
      "    0.80511522]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      "Performing backward propagation on detector layer...\n",
      "\n",
      "[[[-0.10553458  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 1.67194795  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-0.35025905  0.          0.         ...  0.          0.\n",
      "    2.31498593]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.87968239  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 1.50695895  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.95206542\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.85494934  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          2.37841846\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.47641739  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.60153491\n",
      "    0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.05501707  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-1.61362444  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.21158483\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.70435375  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.55067806\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.37763123  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          1.04015805\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.23941848  0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]\n",
      "  [-2.21100132  0.          0.         ...  0.          0.\n",
      "    0.80511522]\n",
      "  [ 0.          0.          0.         ...  0.          0.\n",
      "    0.        ]]]\n",
      "\n",
      "\n",
      "Performing backward propagation on convolution layer...\n"
     ]
    }
   ],
   "source": [
    "error_data = np.array([1.])\n",
    "dE = denseLayer.backward_propagate(error_data)\n",
    "print(dE)\n",
    "print(\"\\n\")\n",
    "dE = flatLayer.backward_propagate(dE)\n",
    "print(dE)\n",
    "print(\"\\n\")\n",
    "dE = poolLayer.backward_propagate(dE)\n",
    "print(dE)\n",
    "print(\"\\n\")\n",
    "dE = detectLayer.backward_propagate(dE)\n",
    "print(dE)\n",
    "print(\"\\n\")\n",
    "dE = convLayer.backward_propagate(dE)\n",
    "print(dE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e01a53af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-07T15:13:29.204032900Z",
     "start_time": "2023-10-07T15:13:28.842590600Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FlattenLayer' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m denseLayer \u001B[38;5;241m=\u001B[39m Model()\u001B[38;5;241m.\u001B[39mDenseLayer(unit_count\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m flatLayer \u001B[38;5;241m=\u001B[39m Model()\u001B[38;5;241m.\u001B[39mFlattenLayer()\n\u001B[1;32m----> 5\u001B[0m flat_f \u001B[38;5;241m=\u001B[39m \u001B[43mflatLayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m(pol_result_f)\n\u001B[0;32m      6\u001B[0m dense_f1 \u001B[38;5;241m=\u001B[39m denseLayer1\u001B[38;5;241m.\u001B[39mdense(flat_f)\n\u001B[0;32m      7\u001B[0m dense_f \u001B[38;5;241m=\u001B[39m denseLayer\u001B[38;5;241m.\u001B[39mdense(dense_f1)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'FlattenLayer' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "#Dense layer testing\n",
    "denseLayer1 = Model().DenseLayer(unit_count=3, activation=\"sigmoid\")\n",
    "denseLayer = Model().DenseLayer(unit_count=1, activation=\"relu\")\n",
    "flatLayer = Model().FlattenLayer()\n",
    "flat_f = flatLayer.flatten(pol_result_f)\n",
    "dense_f1 = denseLayer1.dense(flat_f)\n",
    "dense_f = denseLayer.dense(dense_f1)\n",
    "\n",
    "print(\"bobot awal : \")\n",
    "print(denseLayer._weight)\n",
    "\n",
    "print(\"bias awal : \")\n",
    "print(denseLayer._bias)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"hasil forward1 : \")\n",
    "print(dense_f1)\n",
    "\n",
    "print(\"hasil forward2 : \")\n",
    "print(dense_f)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"hasil backward\")\n",
    "\n",
    "# error_data = np.array([0.1, -0.2, 0.3])\n",
    "error_data = np.array([1.])\n",
    "# Backward propagation\n",
    "dE = denseLayer.backward(error_data)\n",
    "print(\"hasil dE : \")\n",
    "print(dE)\n",
    "\n",
    "dE = denseLayer1.backward(dE)\n",
    "print(\"hasil dE 2: \")\n",
    "print(dE)\n",
    "\n",
    "#Update bobot lapisan\n",
    "# learning_rate = 0.1\n",
    "# denseLayer.update_weight(learning_rate)\n",
    "# print(\"bobot update : \")\n",
    "# print(denseLayer._weight)\n",
    "# print(\"bias update : \")\n",
    "# print(denseLayer._bias)\n",
    "dE = flatLayer.backward(dE)\n",
    "print(\"hasil dE 3: \")\n",
    "print(dE)\n",
    "\n",
    "\n",
    "dE = polingLayer.backward(dE)\n",
    "print(\"hasil dE 4: \")\n",
    "print(dE)\n",
    "\n",
    "dE = detectLayer.backward(dE)\n",
    "print(\"hasil dE 5: \")\n",
    "print(dE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
